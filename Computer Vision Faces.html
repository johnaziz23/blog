<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>CelebA Project—Computer Vision Using Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="Computer Vision Faces_files/libs/clipboard/clipboard.min.js"></script>
<script src="Computer Vision Faces_files/libs/quarto-html/quarto.js"></script>
<script src="Computer Vision Faces_files/libs/quarto-html/popper.min.js"></script>
<script src="Computer Vision Faces_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Computer Vision Faces_files/libs/quarto-html/anchor.min.js"></script>
<link href="Computer Vision Faces_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Computer Vision Faces_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Computer Vision Faces_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Computer Vision Faces_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Computer Vision Faces_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">CelebA Project—Computer Vision Using Machine Learning</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="a-kaggle-project-by-john-aziz" class="level1">
<h1>A Kaggle Project by John Aziz</h1>
<p>This is a personal data science project. I graduated in July 2024 with a Bachelor’s degree in Data Science, and since then I have been building some projects with the intent of improving my skills and being able to secure a job.</p>
<p>I am using a dataset here for machine learning that I obtained from Kaggle—https://www.kaggle.com/datasets/jessicali9530/celeba-dataset</p>
<p>Relevant features of the dataset listed on Kaggle:</p>
<ul>
<li>202,599 number of face images of various celebrities</li>
<li>10,177 unique identities, but names of identities are not given</li>
<li>40 binary attribute annotations per image</li>
</ul>
<p>Everything undertaken here is fairly exploratory work, revisiting and refreshing methods that I used during my undergraduate degree, and trying out some new methods that I have not worked with before.</p>
<p>The first task we are undertaking is to build a binary classifier for smiling or not smiling, a suggested task from the authors of the dataset.</p>
<p>We will develop a convolutional neural network (CNN) using PyTorch and a pre-trained model (MobileNetV2). The goal, as you can probably surmise, is to determine whether the subject in an image is smiling or not.</p>
<p>We use the CelebA dataset, with the ‘Smiling’ attribute as the target. The attribute annotations are loaded and processed, converting labels for binary classification (0 for not smiling, 1 for smiling).</p>
<p>The dataset is split into training, validation, and test sets, and transformed for smaller image dimensions (128x128) to speed up training.</p>
<p>For our model architecture we employ MobileNetV2, a lightweight and efficient model for image classification, fine-tuned for our binary classification task.</p>
<p>The model is trained over 3 epochs using binary cross-entropy loss and optimized using Adam. We employ accuracy and loss metrics to track performance.</p>
<p>The model is validated on a separate dataset after each epoch and finally tested to evaluate its performance.</p>
<div id="5a27671b" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Required Libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms, models</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> tqdm</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Paths</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>image_dir <span class="op">=</span> <span class="st">'/Users/johnaziz/Downloads/archive-3/img_align_celeba/img_align_celeba/'</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>attr_path <span class="op">=</span> <span class="st">'/Users/johnaziz/Downloads/archive-3/list_attr_celeba.csv'</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>partition_path <span class="op">=</span> <span class="st">'/Users/johnaziz/Downloads/archive-3/list_eval_partition.csv'</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the attribute data and partition data</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loading data..."</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>attr_df <span class="op">=</span> pd.read_csv(attr_path)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>partition_df <span class="op">=</span> pd.read_csv(partition_path)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter for the 'Smiling' attribute</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>attr_df[<span class="st">'Smiling'</span>] <span class="op">=</span> attr_df[<span class="st">'Smiling'</span>].replace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>)  <span class="co"># Convert -1 to 0 for binary classification</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Merge partition info into attr_df to facilitate train/val/test split</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Preparing datasets..."</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>attr_df <span class="op">=</span> attr_df.merge(partition_df, on<span class="op">=</span><span class="st">"image_id"</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Define PyTorch Dataset</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CelebADataset(Dataset):</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, img_dir, df, transform<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.img_dir <span class="op">=</span> img_dir</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.df <span class="op">=</span> df</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transform <span class="op">=</span> transform</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.df)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        img_name <span class="op">=</span> os.path.join(<span class="va">self</span>.img_dir, <span class="va">self</span>.df.iloc[idx, <span class="dv">0</span>])</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> Image.<span class="bu">open</span>(img_name)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> torch.tensor(<span class="va">self</span>.df.iloc[idx, <span class="dv">1</span>], dtype<span class="op">=</span>torch.float32)  <span class="co"># Smiling label</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.transform:</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>            image <span class="op">=</span> <span class="va">self</span>.transform(image)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image, label</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Define smaller image transformations (128x128 instead of 224x224)</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    transforms.Resize((<span class="dv">128</span>, <span class="dv">128</span>)),</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>], std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>])</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare the train, validation, and test datasets</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> attr_df[attr_df[<span class="st">'partition'</span>] <span class="op">==</span> <span class="dv">0</span>]  <span class="co"># Training set</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>val_df <span class="op">=</span> attr_df[attr_df[<span class="st">'partition'</span>] <span class="op">==</span> <span class="dv">1</span>]    <span class="co"># Validation set</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>test_df <span class="op">=</span> attr_df[attr_df[<span class="st">'partition'</span>] <span class="op">==</span> <span class="dv">2</span>]   <span class="co"># Test set</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> CelebADataset(image_dir, train_df[[<span class="st">'image_id'</span>, <span class="st">'Smiling'</span>]], transform<span class="op">=</span>transform)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>val_dataset <span class="op">=</span> CelebADataset(image_dir, val_df[[<span class="st">'image_id'</span>, <span class="st">'Smiling'</span>]], transform<span class="op">=</span>transform)</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> CelebADataset(image_dir, test_df[[<span class="st">'image_id'</span>, <span class="st">'Smiling'</span>]], transform<span class="op">=</span>transform)</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="co"># DataLoader for batching (smaller batch size for faster training)</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> DataLoader(test_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a faster model (MobileNetV2) with pretrained weights</span></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SmileClassifier(nn.Module):</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SmileClassifier, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> models.mobilenet_v2(weights<span class="op">=</span><span class="st">'IMAGENET1K_V1'</span>)  <span class="co"># Use MobileNetV2</span></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.classifier[<span class="dv">1</span>] <span class="op">=</span> nn.Linear(<span class="va">self</span>.model.last_channel, <span class="dv">1</span>)  <span class="co"># Binary classification output</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.model(x)</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the model, loss function, and optimizer</span></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SmileClassifier()</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.BCEWithLogitsLoss()  <span class="co"># Binary classification loss</span></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a><span class="co"># Check if GPU is available and set the device</span></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>model.to(device)</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop for 3 epochs (quick training)</span></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'</span><span class="ch">\n</span><span class="ss">Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_epochs<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Training..."</span>)</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Training phase</span></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>        running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>        correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>        total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> images, labels <span class="kw">in</span> tqdm(train_loader):</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>            images, labels <span class="op">=</span> images.to(device), labels.to(device)</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>            labels <span class="op">=</span> labels.unsqueeze(<span class="dv">1</span>)  <span class="co"># Ensure labels are in shape [batch_size, 1]</span></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Forward pass</span></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(images)</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(outputs, labels)</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>            running_loss <span class="op">+=</span> loss.item() <span class="op">*</span> images.size(<span class="dv">0</span>)</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>            preds <span class="op">=</span> torch.sigmoid(outputs).<span class="bu">round</span>()  <span class="co"># Use sigmoid + round for binary classification predictions</span></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>            correct <span class="op">+=</span> (preds <span class="op">==</span> labels).<span class="bu">sum</span>().item()</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>            total <span class="op">+=</span> labels.size(<span class="dv">0</span>)</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>        epoch_loss <span class="op">=</span> running_loss <span class="op">/</span> <span class="bu">len</span>(train_loader.dataset)</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>        epoch_acc <span class="op">=</span> correct <span class="op">/</span> total</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Training Loss: </span><span class="sc">{</span>epoch_loss<span class="sc">:.4f}</span><span class="ss">, Accuracy: </span><span class="sc">{</span>epoch_acc<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Validating..."</span>)</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Validation phase</span></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">eval</span>()</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>        val_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>        val_correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>        val_total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> images, labels <span class="kw">in</span> val_loader:</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>                images, labels <span class="op">=</span> images.to(device), labels.to(device)</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>                labels <span class="op">=</span> labels.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>                outputs <span class="op">=</span> model(images)</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> criterion(outputs, labels)</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>                val_loss <span class="op">+=</span> loss.item() <span class="op">*</span> images.size(<span class="dv">0</span>)</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>                preds <span class="op">=</span> torch.sigmoid(outputs).<span class="bu">round</span>()</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>                val_correct <span class="op">+=</span> (preds <span class="op">==</span> labels).<span class="bu">sum</span>().item()</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>                val_total <span class="op">+=</span> labels.size(<span class="dv">0</span>)</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>        val_epoch_loss <span class="op">=</span> val_loss <span class="op">/</span> <span class="bu">len</span>(val_loader.dataset)</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>        val_epoch_acc <span class="op">=</span> val_correct <span class="op">/</span> val_total</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Validation Loss: </span><span class="sc">{</span>val_epoch_loss<span class="sc">:.4f}</span><span class="ss">, Accuracy: </span><span class="sc">{</span>val_epoch_acc<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Training Complete'</span>)</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model for 3 epochs</span></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluation on the test set</span></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_model(model, test_loader):</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Evaluating the model on the test set..."</span>)</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>    test_correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>    test_total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> images, labels <span class="kw">in</span> test_loader:</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>            images, labels <span class="op">=</span> images.to(device), labels.to(device)</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>            labels <span class="op">=</span> labels.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(images)</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(outputs, labels)</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>            test_loss <span class="op">+=</span> loss.item() <span class="op">*</span> images.size(<span class="dv">0</span>)</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>            preds <span class="op">=</span> torch.sigmoid(outputs).<span class="bu">round</span>()</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>            test_correct <span class="op">+=</span> (preds <span class="op">==</span> labels).<span class="bu">sum</span>().item()</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>            test_total <span class="op">+=</span> labels.size(<span class="dv">0</span>)</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">/=</span> <span class="bu">len</span>(test_loader.dataset)</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>    test_acc <span class="op">=</span> test_correct <span class="op">/</span> test_total</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">:.4f}</span><span class="ss">, Test Accuracy: </span><span class="sc">{</span>test_acc<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model on the test set</span></span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>evaluate_model(model, test_loader)</span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loading data...
Preparing datasets...

Epoch 1/3
Training...</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"e5cd24f11f5044f9ac79393f38809987","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training Loss: 0.1961, Accuracy: 0.9191
Validating...
Validation Loss: 0.1767, Accuracy: 0.9290

Epoch 2/3
Training...</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"9172e7447caf40748d01894b1f65c943","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training Loss: 0.1762, Accuracy: 0.9273
Validating...
Validation Loss: 0.1675, Accuracy: 0.9297

Epoch 3/3
Training...</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"19fac0b4f3be4f9c9eadc42a0f9ae01a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training Loss: 0.1690, Accuracy: 0.9299
Validating...
Validation Loss: 0.1615, Accuracy: 0.9311
Training Complete

Evaluating the model on the test set...
Test Loss: 0.1720, Test Accuracy: 0.9279</code></pre>
</div>
</div>
<p>Throughout the training epochs, both the training and validation losses consistently decreased, while accuracy steadily increased. This indicates that the model was learning effectively and generalizing well on the validation set.</p>
<p>With a test accuracy of 92.79%, we have achieved a strong performance in distinguishing between smiling and non-smiling faces. This is vastly better than 50% which would be achieved by randomly guessing. This means that the model is predicting accurately.</p>
<p>The model’s final performance on the test set shows only a slight increase in loss compared to the validation set, meaning that the model maintained good generalization and performance across unseen data.</p>
<p>Next, we move on to building a predictor for all of the attributes in the dataset.</p>
<p>We once again load the CelebA dataset, merge the attribute and partition data, and convert the attribute labels into binary format.</p>
<p>We define a PyTorch Dataset class that loads the images from the specified directory and returns both the image and the corresponding 40 attribute labels for each sample.</p>
<p>The images are resized to 128x128 to reduce computation and memory usage (especially important given I am training on my laptop) and normalisation is applied for better performance with the pre-trained model.</p>
<p>The model is based on MobileNetV2. The final layer of the model is modified to output 40 logits, each representing one of the 40 attributes in the dataset.</p>
<p>We use Binary Cross Entropy with Logits (BCEWithLogitsLoss) as the loss function, which is suitable for multi-label binary classification.</p>
<p>The model is once again trained for 3 epochs using a batch size of 32.</p>
<p>After training, the model is evaluated on the test set to assess its performance in predicting multiple attributes simultaneously.</p>
<div id="bacc2039" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms, models</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Paths to the data</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>image_dir <span class="op">=</span> <span class="st">'/Users/johnaziz/Downloads/archive-3/img_align_celeba/img_align_celeba/'</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>attr_path <span class="op">=</span> <span class="st">'/Users/johnaziz/Downloads/archive-3/list_attr_celeba.csv'</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>partition_path <span class="op">=</span> <span class="st">'/Users/johnaziz/Downloads/archive-3/list_eval_partition.csv'</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Load attribute and partition data</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loading data..."</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>attr_df <span class="op">=</span> pd.read_csv(attr_path)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>partition_df <span class="op">=</span> pd.read_csv(partition_path)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Merge attribute data with partition data on image_id</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>attr_df <span class="op">=</span> attr_df.merge(partition_df, on<span class="op">=</span><span class="st">"image_id"</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert -1 to 0 for binary classification (0 = attribute absent, 1 = attribute present)</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>attr_df.iloc[:, <span class="dv">1</span>:<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> attr_df.iloc[:, <span class="dv">1</span>:<span class="op">-</span><span class="dv">1</span>].replace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert all columns except 'image_id' and 'partition' to numeric, coercing errors</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> column <span class="kw">in</span> attr_df.columns[<span class="dv">1</span>:<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>    attr_df[column] <span class="op">=</span> pd.to_numeric(attr_df[column], errors<span class="op">=</span><span class="st">'coerce'</span>)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Check for any remaining NaN values after the conversion</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Checking for NaN values in the dataset..."</span>)</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attr_df.isnull().<span class="bu">sum</span>())</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Fill any remaining NaN values with 0 (assuming it's a missing label)</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>attr_df <span class="op">=</span> attr_df.fillna(<span class="dv">0</span>)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify the types again after the fix</span></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Verifying column data types after conversion..."</span>)</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attr_df.dtypes)</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data into train, validation, and test sets based on partition</span></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> attr_df[attr_df[<span class="st">'partition'</span>] <span class="op">==</span> <span class="dv">0</span>]</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>val_df <span class="op">=</span> attr_df[attr_df[<span class="st">'partition'</span>] <span class="op">==</span> <span class="dv">1</span>]</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>test_df <span class="op">=</span> attr_df[attr_df[<span class="st">'partition'</span>] <span class="op">==</span> <span class="dv">2</span>]</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Define PyTorch Dataset for multi-label classification</span></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CelebAMultiLabelDataset(Dataset):</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, img_dir, df, transform<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.img_dir <span class="op">=</span> img_dir</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.df <span class="op">=</span> df</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transform <span class="op">=</span> transform</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.df)</span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>        img_name <span class="op">=</span> os.path.join(<span class="va">self</span>.img_dir, <span class="va">self</span>.df.iloc[idx, <span class="dv">0</span>])</span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> Image.<span class="bu">open</span>(img_name)</span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Updated slice to exclude 'image_id' and 'partition' columns</span></span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> torch.tensor(<span class="va">self</span>.df.iloc[idx, <span class="dv">1</span>:<span class="op">-</span><span class="dv">1</span>].values.astype(<span class="bu">float</span>), dtype<span class="op">=</span>torch.float32)</span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.transform:</span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a>            image <span class="op">=</span> <span class="va">self</span>.transform(image)</span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image, labels</span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a><span class="co"># Define image transformations (128x128 resolution)</span></span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a>    transforms.Resize((<span class="dv">128</span>, <span class="dv">128</span>)),</span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>], std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>])</span>
<span id="cb6-74"><a href="#cb6-74" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb6-75"><a href="#cb6-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare the datasets and DataLoader</span></span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> CelebAMultiLabelDataset(image_dir, train_df, transform<span class="op">=</span>transform)</span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a>val_dataset <span class="op">=</span> CelebAMultiLabelDataset(image_dir, val_df, transform<span class="op">=</span>transform)</span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> CelebAMultiLabelDataset(image_dir, test_df, transform<span class="op">=</span>transform)</span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> DataLoader(test_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-85"><a href="#cb6-85" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model for multi-label classification using MobileNetV2</span></span>
<span id="cb6-86"><a href="#cb6-86" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiLabelAttributeClassifier(nn.Module):</span>
<span id="cb6-87"><a href="#cb6-87" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb6-88"><a href="#cb6-88" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(MultiLabelAttributeClassifier, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb6-89"><a href="#cb6-89" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> models.mobilenet_v2(weights<span class="op">=</span><span class="st">'IMAGENET1K_V1'</span>)  <span class="co"># Load pre-trained weights</span></span>
<span id="cb6-90"><a href="#cb6-90" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.classifier[<span class="dv">1</span>] <span class="op">=</span> nn.Linear(<span class="va">self</span>.model.last_channel, <span class="dv">40</span>)  <span class="co"># 40 attributes to predict</span></span>
<span id="cb6-91"><a href="#cb6-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-92"><a href="#cb6-92" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb6-93"><a href="#cb6-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.model(x)</span>
<span id="cb6-94"><a href="#cb6-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-95"><a href="#cb6-95" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize model, loss function, and optimizer</span></span>
<span id="cb6-96"><a href="#cb6-96" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MultiLabelAttributeClassifier()</span>
<span id="cb6-97"><a href="#cb6-97" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.BCEWithLogitsLoss()  <span class="co"># Binary Cross-Entropy loss for each label</span></span>
<span id="cb6-98"><a href="#cb6-98" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb6-99"><a href="#cb6-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-100"><a href="#cb6-100" aria-hidden="true" tabindex="-1"></a><span class="co"># Check if GPU is available and set the device</span></span>
<span id="cb6-101"><a href="#cb6-101" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb6-102"><a href="#cb6-102" aria-hidden="true" tabindex="-1"></a>model.to(device)</span>
<span id="cb6-103"><a href="#cb6-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-104"><a href="#cb6-104" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to calculate per-attribute accuracy</span></span>
<span id="cb6-105"><a href="#cb6-105" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_per_attribute_accuracy(outputs, labels):</span>
<span id="cb6-106"><a href="#cb6-106" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply sigmoid to the outputs to get probabilities</span></span>
<span id="cb6-107"><a href="#cb6-107" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> torch.sigmoid(outputs)</span>
<span id="cb6-108"><a href="#cb6-108" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-109"><a href="#cb6-109" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert probabilities to binary predictions (threshold at 0.5)</span></span>
<span id="cb6-110"><a href="#cb6-110" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> (probs <span class="op">&gt;</span> <span class="fl">0.5</span>).<span class="bu">float</span>()</span>
<span id="cb6-111"><a href="#cb6-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-112"><a href="#cb6-112" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate per-attribute accuracy</span></span>
<span id="cb6-113"><a href="#cb6-113" aria-hidden="true" tabindex="-1"></a>    correct_per_attribute <span class="op">=</span> (preds <span class="op">==</span> labels).<span class="bu">float</span>().mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-114"><a href="#cb6-114" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-115"><a href="#cb6-115" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> correct_per_attribute</span>
<span id="cb6-116"><a href="#cb6-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-117"><a href="#cb6-117" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop with per-attribute accuracy calculation</span></span>
<span id="cb6-118"><a href="#cb6-118" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model_with_per_attribute_accuracy(model, train_loader, val_loader, criterion, optimizer, num_epochs<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb6-119"><a href="#cb6-119" aria-hidden="true" tabindex="-1"></a>    num_attributes <span class="op">=</span> <span class="dv">40</span>  </span>
<span id="cb6-120"><a href="#cb6-120" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-121"><a href="#cb6-121" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb6-122"><a href="#cb6-122" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'</span><span class="ch">\n</span><span class="ss">Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_epochs<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb6-123"><a href="#cb6-123" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-124"><a href="#cb6-124" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Training phase</span></span>
<span id="cb6-125"><a href="#cb6-125" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb6-126"><a href="#cb6-126" aria-hidden="true" tabindex="-1"></a>        running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb6-127"><a href="#cb6-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-128"><a href="#cb6-128" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> images, labels <span class="kw">in</span> tqdm(train_loader):</span>
<span id="cb6-129"><a href="#cb6-129" aria-hidden="true" tabindex="-1"></a>            images, labels <span class="op">=</span> images.to(device), labels.to(device)</span>
<span id="cb6-130"><a href="#cb6-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-131"><a href="#cb6-131" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb6-132"><a href="#cb6-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-133"><a href="#cb6-133" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Forward pass</span></span>
<span id="cb6-134"><a href="#cb6-134" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(images)</span>
<span id="cb6-135"><a href="#cb6-135" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(outputs, labels)</span>
<span id="cb6-136"><a href="#cb6-136" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb6-137"><a href="#cb6-137" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb6-138"><a href="#cb6-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-139"><a href="#cb6-139" aria-hidden="true" tabindex="-1"></a>            running_loss <span class="op">+=</span> loss.item() <span class="op">*</span> images.size(<span class="dv">0</span>)</span>
<span id="cb6-140"><a href="#cb6-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-141"><a href="#cb6-141" aria-hidden="true" tabindex="-1"></a>        epoch_loss <span class="op">=</span> running_loss <span class="op">/</span> <span class="bu">len</span>(train_loader.dataset)</span>
<span id="cb6-142"><a href="#cb6-142" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Training Loss: </span><span class="sc">{</span>epoch_loss<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb6-143"><a href="#cb6-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-144"><a href="#cb6-144" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Validation phase</span></span>
<span id="cb6-145"><a href="#cb6-145" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">eval</span>()</span>
<span id="cb6-146"><a href="#cb6-146" aria-hidden="true" tabindex="-1"></a>        val_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb6-147"><a href="#cb6-147" aria-hidden="true" tabindex="-1"></a>        total_correct_per_attribute <span class="op">=</span> torch.zeros(num_attributes).to(device)  <span class="co"># Track accuracy for each attribute</span></span>
<span id="cb6-148"><a href="#cb6-148" aria-hidden="true" tabindex="-1"></a>        total_batches <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-149"><a href="#cb6-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-150"><a href="#cb6-150" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb6-151"><a href="#cb6-151" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> images, labels <span class="kw">in</span> val_loader:</span>
<span id="cb6-152"><a href="#cb6-152" aria-hidden="true" tabindex="-1"></a>                images, labels <span class="op">=</span> images.to(device), labels.to(device)</span>
<span id="cb6-153"><a href="#cb6-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-154"><a href="#cb6-154" aria-hidden="true" tabindex="-1"></a>                outputs <span class="op">=</span> model(images)</span>
<span id="cb6-155"><a href="#cb6-155" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> criterion(outputs, labels)</span>
<span id="cb6-156"><a href="#cb6-156" aria-hidden="true" tabindex="-1"></a>                val_loss <span class="op">+=</span> loss.item() <span class="op">*</span> images.size(<span class="dv">0</span>)</span>
<span id="cb6-157"><a href="#cb6-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-158"><a href="#cb6-158" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Calculate accuracy per attribute for this batch</span></span>
<span id="cb6-159"><a href="#cb6-159" aria-hidden="true" tabindex="-1"></a>                correct_per_attribute <span class="op">=</span> calculate_per_attribute_accuracy(outputs, labels)</span>
<span id="cb6-160"><a href="#cb6-160" aria-hidden="true" tabindex="-1"></a>                total_correct_per_attribute <span class="op">+=</span> correct_per_attribute</span>
<span id="cb6-161"><a href="#cb6-161" aria-hidden="true" tabindex="-1"></a>                total_batches <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb6-162"><a href="#cb6-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-163"><a href="#cb6-163" aria-hidden="true" tabindex="-1"></a>        val_epoch_loss <span class="op">=</span> val_loss <span class="op">/</span> <span class="bu">len</span>(val_loader.dataset)</span>
<span id="cb6-164"><a href="#cb6-164" aria-hidden="true" tabindex="-1"></a>        per_attribute_accuracy <span class="op">=</span> total_correct_per_attribute <span class="op">/</span> total_batches</span>
<span id="cb6-165"><a href="#cb6-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-166"><a href="#cb6-166" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Validation Loss: </span><span class="sc">{</span>val_epoch_loss<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb6-167"><a href="#cb6-167" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Per-Attribute Validation Accuracy: </span><span class="sc">{</span>per_attribute_accuracy<span class="sc">.</span>cpu()<span class="sc">.</span>numpy()<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb6-168"><a href="#cb6-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-169"><a href="#cb6-169" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Training complete'</span>)</span>
<span id="cb6-170"><a href="#cb6-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-171"><a href="#cb6-171" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model with per-attribute validation accuracy</span></span>
<span id="cb6-172"><a href="#cb6-172" aria-hidden="true" tabindex="-1"></a>train_model_with_per_attribute_accuracy(model, train_loader, val_loader, criterion, optimizer, num_epochs<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb6-173"><a href="#cb6-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-174"><a href="#cb6-174" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluation on the test set (if needed)</span></span>
<span id="cb6-175"><a href="#cb6-175" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_model(model, test_loader):</span>
<span id="cb6-176"><a href="#cb6-176" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Evaluating the model on the test set..."</span>)</span>
<span id="cb6-177"><a href="#cb6-177" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb6-178"><a href="#cb6-178" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb6-179"><a href="#cb6-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-180"><a href="#cb6-180" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb6-181"><a href="#cb6-181" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> images, labels <span class="kw">in</span> test_loader:</span>
<span id="cb6-182"><a href="#cb6-182" aria-hidden="true" tabindex="-1"></a>            images, labels <span class="op">=</span> images.to(device), labels.to(device)</span>
<span id="cb6-183"><a href="#cb6-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-184"><a href="#cb6-184" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(images)</span>
<span id="cb6-185"><a href="#cb6-185" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(outputs, labels)</span>
<span id="cb6-186"><a href="#cb6-186" aria-hidden="true" tabindex="-1"></a>            test_loss <span class="op">+=</span> loss.item() <span class="op">*</span> images.size(<span class="dv">0</span>)</span>
<span id="cb6-187"><a href="#cb6-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-188"><a href="#cb6-188" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">/=</span> <span class="bu">len</span>(test_loader.dataset)</span>
<span id="cb6-189"><a href="#cb6-189" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb6-190"><a href="#cb6-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-191"><a href="#cb6-191" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model on the test set</span></span>
<span id="cb6-192"><a href="#cb6-192" aria-hidden="true" tabindex="-1"></a>evaluate_model(model, test_loader)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loading data...
Checking for NaN values in the dataset...
image_id               0
5_o_Clock_Shadow       0
Arched_Eyebrows        0
Attractive             0
Bags_Under_Eyes        0
Bald                   0
Bangs                  0
Big_Lips               0
Big_Nose               0
Black_Hair             0
Blond_Hair             0
Blurry                 0
Brown_Hair             0
Bushy_Eyebrows         0
Chubby                 0
Double_Chin            0
Eyeglasses             0
Goatee                 0
Gray_Hair              0
Heavy_Makeup           0
High_Cheekbones        0
Male                   0
Mouth_Slightly_Open    0
Mustache               0
Narrow_Eyes            0
No_Beard               0
Oval_Face              0
Pale_Skin              0
Pointy_Nose            0
Receding_Hairline      0
Rosy_Cheeks            0
Sideburns              0
Smiling                0
Straight_Hair          0
Wavy_Hair              0
Wearing_Earrings       0
Wearing_Hat            0
Wearing_Lipstick       0
Wearing_Necklace       0
Wearing_Necktie        0
Young                  0
partition              0
dtype: int64
Verifying column data types after conversion...
image_id               object
5_o_Clock_Shadow        int64
Arched_Eyebrows         int64
Attractive              int64
Bags_Under_Eyes         int64
Bald                    int64
Bangs                   int64
Big_Lips                int64
Big_Nose                int64
Black_Hair              int64
Blond_Hair              int64
Blurry                  int64
Brown_Hair              int64
Bushy_Eyebrows          int64
Chubby                  int64
Double_Chin             int64
Eyeglasses              int64
Goatee                  int64
Gray_Hair               int64
Heavy_Makeup            int64
High_Cheekbones         int64
Male                    int64
Mouth_Slightly_Open     int64
Mustache                int64
Narrow_Eyes             int64
No_Beard                int64
Oval_Face               int64
Pale_Skin               int64
Pointy_Nose             int64
Receding_Hairline       int64
Rosy_Cheeks             int64
Sideburns               int64
Smiling                 int64
Straight_Hair           int64
Wavy_Hair               int64
Wearing_Earrings        int64
Wearing_Hat             int64
Wearing_Lipstick        int64
Wearing_Necklace        int64
Wearing_Necktie         int64
Young                   int64
partition               int64
dtype: object

Epoch 1/3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|███████████████████████████████████████| 5087/5087 [57:02&lt;00:00,  1.49it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training Loss: 0.2272
Validation Loss: 0.2063
Per-Attribute Validation Accuracy: [0.9354778  0.8435021  0.8092104  0.8323903  0.9883756  0.9565124
 0.81417364 0.8181081  0.8700815  0.939101   0.9642117  0.8301891
 0.92022085 0.94774705 0.96062946 0.9916465  0.95958203 0.9722725
 0.9110716  0.8751137  0.98152244 0.9376734  0.955547   0.9385792
 0.95827365 0.75376856 0.96738195 0.75965625 0.94039077 0.94457674
 0.9609407  0.92246675 0.8345131  0.8601271  0.9052845  0.9853469
 0.9236335  0.8854073  0.96164525 0.8600953 ]

Epoch 2/3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|███████████████████████████████████████| 5087/5087 [58:05&lt;00:00,  1.46it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training Loss: 0.2052
Validation Loss: 0.1994
Per-Attribute Validation Accuracy: [0.9381355  0.8535852  0.805207   0.82777    0.98806435 0.9545908
 0.8556167  0.82208353 0.86395156 0.9518734  0.9643533  0.84239507
 0.9223344  0.94940764 0.9621988  0.99541134 0.96592265 0.9768015
 0.91328573 0.8661564  0.9811292  0.9353771  0.9591198  0.9387805
 0.95681435 0.7545234  0.9623498  0.7628582  0.9434604  0.9436803
 0.9683381  0.927348   0.837907   0.8657128  0.9104584  0.9889608
 0.9249922  0.88690764 0.9647149  0.8743775 ]

Epoch 3/3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|███████████████████████████████████████| 5087/5087 [53:30&lt;00:00,  1.58it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training Loss: 0.1981
Validation Loss: 0.1932
Per-Attribute Validation Accuracy: [0.93954456 0.85644424 0.8130256  0.8421714  0.989684   0.9578114
 0.85269797 0.8270244  0.9111537  0.9530812  0.96284366 0.8554881
 0.927074   0.9556476  0.96481556 0.9933072  0.9660643  0.9781602
 0.9225264  0.87979364 0.983595   0.9391923  0.95906025 0.9133864
 0.9594907  0.76101494 0.9608401  0.75714016 0.9443159  0.94544154
 0.9701497  0.9240174  0.8425236  0.8602873  0.90820324 0.98897016
 0.9297225  0.8905812  0.9644633  0.87331146]
Training complete

Evaluating the model on the test set...
Test Loss: 0.2011</code></pre>
</div>
</div>
<p>However, I realised that this formatting was not displaying the data very informatively, so because I did not want to wait to train the model again as it was taking hours per run, so I used pandas to create a more visually readable table:</p>
<div id="9e7f036f" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Attribute names </span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>attribute_names <span class="op">=</span> [</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"5_o_Clock_Shadow"</span>, <span class="st">"Arched_Eyebrows"</span>, <span class="st">"Attractive"</span>, <span class="st">"Bags_Under_Eyes"</span>,</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Bald"</span>, <span class="st">"Bangs"</span>, <span class="st">"Big_Lips"</span>, <span class="st">"Big_Nose"</span>, <span class="st">"Black_Hair"</span>, <span class="st">"Blond_Hair"</span>,</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Blurry"</span>, <span class="st">"Brown_Hair"</span>, <span class="st">"Bushy_Eyebrows"</span>, <span class="st">"Chubby"</span>, <span class="st">"Double_Chin"</span>,</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Eyeglasses"</span>, <span class="st">"Goatee"</span>, <span class="st">"Gray_Hair"</span>, <span class="st">"Heavy_Makeup"</span>, <span class="st">"High_Cheekbones"</span>,</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Male"</span>, <span class="st">"Mouth_Slightly_Open"</span>, <span class="st">"Mustache"</span>, <span class="st">"Narrow_Eyes"</span>, <span class="st">"No_Beard"</span>,</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Oval_Face"</span>, <span class="st">"Pale_Skin"</span>, <span class="st">"Pointy_Nose"</span>, <span class="st">"Receding_Hairline"</span>, <span class="st">"Rosy_Cheeks"</span>,</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Sideburns"</span>, <span class="st">"Smiling"</span>, <span class="st">"Straight_Hair"</span>, <span class="st">"Wavy_Hair"</span>, <span class="st">"Wearing_Earrings"</span>,</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Wearing_Hat"</span>, <span class="st">"Wearing_Lipstick"</span>, <span class="st">"Wearing_Necklace"</span>, <span class="st">"Wearing_Necktie"</span>,</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Young"</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Validation accuracies for each epoch</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>epoch_1_accuracy <span class="op">=</span> [<span class="fl">0.9354778</span>, <span class="fl">0.8435021</span>, <span class="fl">0.8092104</span>, <span class="fl">0.8323903</span>, <span class="fl">0.9883756</span>, <span class="fl">0.9565124</span>,</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">0.81417364</span>, <span class="fl">0.8181081</span>, <span class="fl">0.8700815</span>, <span class="fl">0.939101</span>, <span class="fl">0.9642117</span>, <span class="fl">0.8301891</span>,</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">0.92022085</span>, <span class="fl">0.94774705</span>, <span class="fl">0.96062946</span>, <span class="fl">0.9916465</span>, <span class="fl">0.95958203</span>, <span class="fl">0.9722725</span>,</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">0.9110716</span>, <span class="fl">0.8751137</span>, <span class="fl">0.98152244</span>, <span class="fl">0.9376734</span>, <span class="fl">0.955547</span>, <span class="fl">0.9385792</span>,</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">0.95827365</span>, <span class="fl">0.75376856</span>, <span class="fl">0.96738195</span>, <span class="fl">0.75965625</span>, <span class="fl">0.94039077</span>, <span class="fl">0.94457674</span>,</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">0.9609407</span>, <span class="fl">0.92246675</span>, <span class="fl">0.8345131</span>, <span class="fl">0.8601271</span>, <span class="fl">0.9052845</span>, <span class="fl">0.9853469</span>,</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">0.9236335</span>, <span class="fl">0.8854073</span>, <span class="fl">0.96164525</span>, <span class="fl">0.8600953</span>]</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>epoch_2_accuracy <span class="op">=</span> [<span class="fl">0.9381355</span>, <span class="fl">0.8535852</span>, <span class="fl">0.805207</span>, <span class="fl">0.82777</span>, <span class="fl">0.98806435</span>, <span class="fl">0.9545908</span>,</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">0.8556167</span>, <span class="fl">0.82208353</span>, <span class="fl">0.86395156</span>, <span class="fl">0.9518734</span>, <span class="fl">0.9643533</span>, <span class="fl">0.84239507</span>,</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">0.9223344</span>, <span class="fl">0.94940764</span>, <span class="fl">0.9621988</span>, <span class="fl">0.99541134</span>, <span class="fl">0.96592265</span>, <span class="fl">0.9768015</span>,</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">0.91328573</span>, <span class="fl">0.8661564</span>, <span class="fl">0.9811292</span>, <span class="fl">0.9353771</span>, <span class="fl">0.9591198</span>, <span class="fl">0.9387805</span>,</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">0.95681435</span>, <span class="fl">0.7545234</span>, <span class="fl">0.9623498</span>, <span class="fl">0.7628582</span>, <span class="fl">0.9434604</span>, <span class="fl">0.9436803</span>,</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">0.9683381</span>, <span class="fl">0.927348</span>, <span class="fl">0.837907</span>, <span class="fl">0.8657128</span>, <span class="fl">0.9104584</span>, <span class="fl">0.9889608</span>,</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">0.9249922</span>, <span class="fl">0.88690764</span>, <span class="fl">0.9647149</span>, <span class="fl">0.8743775</span>]</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>epoch_3_accuracy <span class="op">=</span> [<span class="fl">0.93954456</span>, <span class="fl">0.85644424</span>, <span class="fl">0.8130256</span>, <span class="fl">0.8421714</span>, <span class="fl">0.989684</span>, <span class="fl">0.9578114</span>,</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">0.85269797</span>, <span class="fl">0.8270244</span>, <span class="fl">0.9111537</span>, <span class="fl">0.9530812</span>, <span class="fl">0.96284366</span>, <span class="fl">0.8554881</span>,</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">0.927074</span>, <span class="fl">0.9556476</span>, <span class="fl">0.96481556</span>, <span class="fl">0.9933072</span>, <span class="fl">0.9660643</span>, <span class="fl">0.9781602</span>,</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">0.9225264</span>, <span class="fl">0.87979364</span>, <span class="fl">0.983595</span>, <span class="fl">0.9391923</span>, <span class="fl">0.95906025</span>, <span class="fl">0.9133864</span>,</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">0.9594907</span>, <span class="fl">0.76101494</span>, <span class="fl">0.9608401</span>, <span class="fl">0.75714016</span>, <span class="fl">0.9443159</span>, <span class="fl">0.94544154</span>,</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">0.9701497</span>, <span class="fl">0.9240174</span>, <span class="fl">0.8425236</span>, <span class="fl">0.8602873</span>, <span class="fl">0.90820324</span>, <span class="fl">0.98897016</span>,</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">0.9297225</span>, <span class="fl">0.8905812</span>, <span class="fl">0.9644633</span>, <span class="fl">0.87331146</span>]</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrame</span></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Attribute'</span>: attribute_names,</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Epoch 1 Accuracy'</span>: epoch_1_accuracy,</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Epoch 2 Accuracy'</span>: epoch_2_accuracy,</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Epoch 3 Accuracy'</span>: epoch_3_accuracy</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the DataFrame</span></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Attribute</th>
<th data-quarto-table-cell-role="th">Epoch 1 Accuracy</th>
<th data-quarto-table-cell-role="th">Epoch 2 Accuracy</th>
<th data-quarto-table-cell-role="th">Epoch 3 Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>5_o_Clock_Shadow</td>
<td>0.935478</td>
<td>0.938136</td>
<td>0.939545</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Arched_Eyebrows</td>
<td>0.843502</td>
<td>0.853585</td>
<td>0.856444</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Attractive</td>
<td>0.809210</td>
<td>0.805207</td>
<td>0.813026</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>Bags_Under_Eyes</td>
<td>0.832390</td>
<td>0.827770</td>
<td>0.842171</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>Bald</td>
<td>0.988376</td>
<td>0.988064</td>
<td>0.989684</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>Bangs</td>
<td>0.956512</td>
<td>0.954591</td>
<td>0.957811</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>Big_Lips</td>
<td>0.814174</td>
<td>0.855617</td>
<td>0.852698</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>Big_Nose</td>
<td>0.818108</td>
<td>0.822084</td>
<td>0.827024</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>Black_Hair</td>
<td>0.870081</td>
<td>0.863952</td>
<td>0.911154</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>Blond_Hair</td>
<td>0.939101</td>
<td>0.951873</td>
<td>0.953081</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">10</td>
<td>Blurry</td>
<td>0.964212</td>
<td>0.964353</td>
<td>0.962844</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">11</td>
<td>Brown_Hair</td>
<td>0.830189</td>
<td>0.842395</td>
<td>0.855488</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12</td>
<td>Bushy_Eyebrows</td>
<td>0.920221</td>
<td>0.922334</td>
<td>0.927074</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">13</td>
<td>Chubby</td>
<td>0.947747</td>
<td>0.949408</td>
<td>0.955648</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">14</td>
<td>Double_Chin</td>
<td>0.960629</td>
<td>0.962199</td>
<td>0.964816</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15</td>
<td>Eyeglasses</td>
<td>0.991646</td>
<td>0.995411</td>
<td>0.993307</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">16</td>
<td>Goatee</td>
<td>0.959582</td>
<td>0.965923</td>
<td>0.966064</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">17</td>
<td>Gray_Hair</td>
<td>0.972272</td>
<td>0.976801</td>
<td>0.978160</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">18</td>
<td>Heavy_Makeup</td>
<td>0.911072</td>
<td>0.913286</td>
<td>0.922526</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">19</td>
<td>High_Cheekbones</td>
<td>0.875114</td>
<td>0.866156</td>
<td>0.879794</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">20</td>
<td>Male</td>
<td>0.981522</td>
<td>0.981129</td>
<td>0.983595</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">21</td>
<td>Mouth_Slightly_Open</td>
<td>0.937673</td>
<td>0.935377</td>
<td>0.939192</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">22</td>
<td>Mustache</td>
<td>0.955547</td>
<td>0.959120</td>
<td>0.959060</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">23</td>
<td>Narrow_Eyes</td>
<td>0.938579</td>
<td>0.938781</td>
<td>0.913386</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">24</td>
<td>No_Beard</td>
<td>0.958274</td>
<td>0.956814</td>
<td>0.959491</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">25</td>
<td>Oval_Face</td>
<td>0.753769</td>
<td>0.754523</td>
<td>0.761015</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">26</td>
<td>Pale_Skin</td>
<td>0.967382</td>
<td>0.962350</td>
<td>0.960840</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">27</td>
<td>Pointy_Nose</td>
<td>0.759656</td>
<td>0.762858</td>
<td>0.757140</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">28</td>
<td>Receding_Hairline</td>
<td>0.940391</td>
<td>0.943460</td>
<td>0.944316</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">29</td>
<td>Rosy_Cheeks</td>
<td>0.944577</td>
<td>0.943680</td>
<td>0.945442</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">30</td>
<td>Sideburns</td>
<td>0.960941</td>
<td>0.968338</td>
<td>0.970150</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">31</td>
<td>Smiling</td>
<td>0.922467</td>
<td>0.927348</td>
<td>0.924017</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">32</td>
<td>Straight_Hair</td>
<td>0.834513</td>
<td>0.837907</td>
<td>0.842524</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">33</td>
<td>Wavy_Hair</td>
<td>0.860127</td>
<td>0.865713</td>
<td>0.860287</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">34</td>
<td>Wearing_Earrings</td>
<td>0.905285</td>
<td>0.910458</td>
<td>0.908203</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">35</td>
<td>Wearing_Hat</td>
<td>0.985347</td>
<td>0.988961</td>
<td>0.988970</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">36</td>
<td>Wearing_Lipstick</td>
<td>0.923633</td>
<td>0.924992</td>
<td>0.929723</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">37</td>
<td>Wearing_Necklace</td>
<td>0.885407</td>
<td>0.886908</td>
<td>0.890581</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">38</td>
<td>Wearing_Necktie</td>
<td>0.961645</td>
<td>0.964715</td>
<td>0.964463</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">39</td>
<td>Young</td>
<td>0.860095</td>
<td>0.874378</td>
<td>0.873311</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Certain attributes like glasses (99.33%), male (98.36%), and bald (98.97%) achieved consistently high accuracy across all epochs. These attributes tend to have visually distinct features that the model could make out, making them easier to predict accurately.</p>
<p>Some attributes like Attractive (81.30%), Oval Face (76.10%), and Pointy Nose (75.71%) had lower accuracy. These features would seem to be more subjective (indeed, up to the discretion of those who labelled the dataset) or harder to detect, making them more difficult for the model to predict accurately.</p>
<p>Most attributes showed steady improvement across the epochs, with small increases in accuracy.</p>
<p>Next, we moved onto using Logistic Regression to model the presence of multiple facial attributes in the CelebA dataset. The goal is to assess the conditional probability of each attribute, predicting whether a given attribute is present based on other features. For each of the 40 attributes, we train a logistic regression classifier and evaluate its performance using accuracy, precision, recall, and F1-score.</p>
<p>Logistic regression is a suitable choice for this multi-label binary classification task, allowing us to understand how each attribute correlates with the input features.</p>
<div id="4307d871" class="cell" data-tags="[]" data-execution_count="30">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, precision_score, recall_score, f1_score</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Paths to data</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>image_dir <span class="op">=</span> <span class="st">'/Users/johnaziz/Downloads/archive-3/img_align_celeba/img_align_celeba/'</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>attr_path <span class="op">=</span> <span class="st">'/Users/johnaziz/Downloads/archive-3/list_attr_celeba.csv'</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>partition_path <span class="op">=</span> <span class="st">'/Users/johnaziz/Downloads/archive-3/list_eval_partition.csv'</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Load attribute and partition data</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loading data..."</span>)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>attr_df <span class="op">=</span> pd.read_csv(attr_path)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>partition_df <span class="op">=</span> pd.read_csv(partition_path)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Merge attribute data with partition data on image_id</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> attr_df.merge(partition_df, on<span class="op">=</span><span class="st">"image_id"</span>)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert -1 to 0 for binary classification (0 = attribute absent, 1 = attribute present)</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>train_df.iloc[:, <span class="dv">1</span>:<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> train_df.iloc[:, <span class="dv">1</span>:<span class="op">-</span><span class="dv">1</span>].replace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Attribute names</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>attribute_names <span class="op">=</span> [</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">"5_o_Clock_Shadow"</span>, <span class="st">"Arched_Eyebrows"</span>, <span class="st">"Attractive"</span>, <span class="st">"Bags_Under_Eyes"</span>,</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Bald"</span>, <span class="st">"Bangs"</span>, <span class="st">"Big_Lips"</span>, <span class="st">"Big_Nose"</span>, <span class="st">"Black_Hair"</span>, <span class="st">"Blond_Hair"</span>,</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Blurry"</span>, <span class="st">"Brown_Hair"</span>, <span class="st">"Bushy_Eyebrows"</span>, <span class="st">"Chubby"</span>, <span class="st">"Double_Chin"</span>,</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Eyeglasses"</span>, <span class="st">"Goatee"</span>, <span class="st">"Gray_Hair"</span>, <span class="st">"Heavy_Makeup"</span>, <span class="st">"High_Cheekbones"</span>,</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Male"</span>, <span class="st">"Mouth_Slightly_Open"</span>, <span class="st">"Mustache"</span>, <span class="st">"Narrow_Eyes"</span>, <span class="st">"No_Beard"</span>,</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Oval_Face"</span>, <span class="st">"Pale_Skin"</span>, <span class="st">"Pointy_Nose"</span>, <span class="st">"Receding_Hairline"</span>, <span class="st">"Rosy_Cheeks"</span>,</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Sideburns"</span>, <span class="st">"Smiling"</span>, <span class="st">"Straight_Hair"</span>, <span class="st">"Wavy_Hair"</span>, <span class="st">"Wearing_Earrings"</span>,</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Wearing_Hat"</span>, <span class="st">"Wearing_Lipstick"</span>, <span class="st">"Wearing_Necklace"</span>, <span class="st">"Wearing_Necktie"</span>,</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Young"</span></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare an empty DataFrame to store the logistic regression results</span></span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>logreg_results <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">'Attribute'</span>, <span class="st">'Accuracy'</span>, <span class="st">'Precision'</span>, <span class="st">'Recall'</span>, <span class="st">'F1-Score'</span>])</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate over each attribute to train logistic regression models and evaluate them</span></span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, attribute <span class="kw">in</span> <span class="bu">enumerate</span>(attribute_names):</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Processing </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span><span class="bu">len</span>(attribute_names)<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>attribute<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Prepare the dataset: use all attributes except the target as features</span></span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> train_df.drop(columns<span class="op">=</span>[<span class="st">'image_id'</span>, <span class="st">'partition'</span>, attribute])</span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> train_df[attribute]</span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train/test split</span></span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>    X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Data split complete. Training Logistic Regression model for '</span><span class="sc">{</span>attribute<span class="sc">}</span><span class="ss">'..."</span>)</span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train a logistic regression model</span></span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> LogisticRegression(max_iter<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a>        model.fit(X_train, y_train)</span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Error training model for </span><span class="sc">{</span>attribute<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make predictions</span></span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate performance metrics</span></span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a>    accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a>    precision <span class="op">=</span> precision_score(y_test, y_pred, zero_division<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true" tabindex="-1"></a>    recall <span class="op">=</span> recall_score(y_test, y_pred, zero_division<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true" tabindex="-1"></a>    f1 <span class="op">=</span> f1_score(y_test, y_pred, zero_division<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Completed </span><span class="sc">{</span>attribute<span class="sc">}</span><span class="ss">. Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.4f}</span><span class="ss">, Precision: </span><span class="sc">{</span>precision<span class="sc">:.4f}</span><span class="ss">, Recall: </span><span class="sc">{</span>recall<span class="sc">:.4f}</span><span class="ss">, F1-Score: </span><span class="sc">{</span>f1<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-70"><a href="#cb15-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a temporary DataFrame to store the results for the current attribute</span></span>
<span id="cb15-71"><a href="#cb15-71" aria-hidden="true" tabindex="-1"></a>    temp_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb15-72"><a href="#cb15-72" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Attribute'</span>: [attribute],</span>
<span id="cb15-73"><a href="#cb15-73" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Accuracy'</span>: [accuracy],</span>
<span id="cb15-74"><a href="#cb15-74" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Precision'</span>: [precision],</span>
<span id="cb15-75"><a href="#cb15-75" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Recall'</span>: [recall],</span>
<span id="cb15-76"><a href="#cb15-76" aria-hidden="true" tabindex="-1"></a>        <span class="st">'F1-Score'</span>: [f1]</span>
<span id="cb15-77"><a href="#cb15-77" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb15-78"><a href="#cb15-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-79"><a href="#cb15-79" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Concatenate the temp_df to logreg_results using pd.concat</span></span>
<span id="cb15-80"><a href="#cb15-80" aria-hidden="true" tabindex="-1"></a>    logreg_results <span class="op">=</span> pd.concat([logreg_results, temp_df], ignore_index<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-81"><a href="#cb15-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-82"><a href="#cb15-82" aria-hidden="true" tabindex="-1"></a><span class="co"># Now let's move to the conditional probability calculation</span></span>
<span id="cb15-83"><a href="#cb15-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-84"><a href="#cb15-84" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize an empty DataFrame to store conditional probabilities</span></span>
<span id="cb15-85"><a href="#cb15-85" aria-hidden="true" tabindex="-1"></a>conditional_probs <span class="op">=</span> pd.DataFrame(index<span class="op">=</span>attribute_names, columns<span class="op">=</span>attribute_names)</span>
<span id="cb15-86"><a href="#cb15-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-87"><a href="#cb15-87" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate over each attribute pair and calculate conditional probabilities</span></span>
<span id="cb15-88"><a href="#cb15-88" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> feature_a <span class="kw">in</span> attribute_names:</span>
<span id="cb15-89"><a href="#cb15-89" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> feature_b <span class="kw">in</span> attribute_names:</span>
<span id="cb15-90"><a href="#cb15-90" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate P(A | B), where A is feature_a and B is feature_b</span></span>
<span id="cb15-91"><a href="#cb15-91" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> feature_a <span class="op">!=</span> feature_b:</span>
<span id="cb15-92"><a href="#cb15-92" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calculate joint probability P(A ∩ B)</span></span>
<span id="cb15-93"><a href="#cb15-93" aria-hidden="true" tabindex="-1"></a>            joint_prob <span class="op">=</span> ((train_df[feature_a] <span class="op">==</span> <span class="dv">1</span>) <span class="op">&amp;</span> (train_df[feature_b] <span class="op">==</span> <span class="dv">1</span>)).mean()  </span>
<span id="cb15-94"><a href="#cb15-94" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calculate marginal probability P(B)</span></span>
<span id="cb15-95"><a href="#cb15-95" aria-hidden="true" tabindex="-1"></a>            prob_b <span class="op">=</span> (train_df[feature_b] <span class="op">==</span> <span class="dv">1</span>).mean()</span>
<span id="cb15-96"><a href="#cb15-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-97"><a href="#cb15-97" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Avoid division by zero</span></span>
<span id="cb15-98"><a href="#cb15-98" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> prob_b <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb15-99"><a href="#cb15-99" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Calculate conditional probability P(A | B)</span></span>
<span id="cb15-100"><a href="#cb15-100" aria-hidden="true" tabindex="-1"></a>                conditional_probs.at[feature_a, feature_b] <span class="op">=</span> joint_prob <span class="op">/</span> prob_b  </span>
<span id="cb15-101"><a href="#cb15-101" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb15-102"><a href="#cb15-102" aria-hidden="true" tabindex="-1"></a>                conditional_probs.at[feature_a, feature_b] <span class="op">=</span> <span class="va">None</span>  <span class="co"># Undefined if P(B) == 0</span></span>
<span id="cb15-103"><a href="#cb15-103" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb15-104"><a href="#cb15-104" aria-hidden="true" tabindex="-1"></a>            <span class="co"># P(A | A) = 1</span></span>
<span id="cb15-105"><a href="#cb15-105" aria-hidden="true" tabindex="-1"></a>            conditional_probs.at[feature_a, feature_b] <span class="op">=</span> <span class="fl">1.0</span>  </span>
<span id="cb15-106"><a href="#cb15-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-107"><a href="#cb15-107" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the logistic regression results</span></span>
<span id="cb15-108"><a href="#cb15-108" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Logistic Regression Results:"</span>)</span>
<span id="cb15-109"><a href="#cb15-109" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(logreg_results)</span>
<span id="cb15-110"><a href="#cb15-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-111"><a href="#cb15-111" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the conditional probabilities table</span></span>
<span id="cb15-112"><a href="#cb15-112" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Conditional Probability Table:"</span>)</span>
<span id="cb15-113"><a href="#cb15-113" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conditional_probs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loading data...
Processing 1/40: 5_o_Clock_Shadow
  Data split complete. Training Logistic Regression model for '5_o_Clock_Shadow'...
  Completed 5_o_Clock_Shadow. Accuracy: 0.9260, Precision: 0.7235, Recall: 0.5325, F1-Score: 0.6135
Processing 2/40: Arched_Eyebrows
  Data split complete. Training Logistic Regression model for 'Arched_Eyebrows'...
  Completed Arched_Eyebrows. Accuracy: 0.8024, Precision: 0.6660, Recall: 0.5221, F1-Score: 0.5853
Processing 3/40: Attractive
  Data split complete. Training Logistic Regression model for 'Attractive'...
  Completed Attractive. Accuracy: 0.7862, Precision: 0.7783, Recall: 0.8132, F1-Score: 0.7954
Processing 4/40: Bags_Under_Eyes
  Data split complete. Training Logistic Regression model for 'Bags_Under_Eyes'...
  Completed Bags_Under_Eyes. Accuracy: 0.8327, Precision: 0.6577, Recall: 0.3975, F1-Score: 0.4955
Processing 5/40: Bald
  Data split complete. Training Logistic Regression model for 'Bald'...
  Completed Bald. Accuracy: 0.9781, Precision: 0.5485, Recall: 0.1243, F1-Score: 0.2027
Processing 6/40: Bangs
  Data split complete. Training Logistic Regression model for 'Bangs'...
  Completed Bangs. Accuracy: 0.8471, Precision: 0.6180, Recall: 0.0264, F1-Score: 0.0506
Processing 7/40: Big_Lips
  Data split complete. Training Logistic Regression model for 'Big_Lips'...
  Completed Big_Lips. Accuracy: 0.7740, Precision: 0.5880, Recall: 0.2170, F1-Score: 0.3170
Processing 8/40: Big_Nose
  Data split complete. Training Logistic Regression model for 'Big_Nose'...
  Completed Big_Nose. Accuracy: 0.8328, Precision: 0.7050, Recall: 0.5019, F1-Score: 0.5864
Processing 9/40: Black_Hair
  Data split complete. Training Logistic Regression model for 'Black_Hair'...
  Completed Black_Hair. Accuracy: 0.8007, Precision: 0.6251, Recall: 0.4077, F1-Score: 0.4935
Processing 10/40: Blond_Hair
  Data split complete. Training Logistic Regression model for 'Blond_Hair'...
  Completed Blond_Hair. Accuracy: 0.8674, Precision: 0.5866, Recall: 0.3654, F1-Score: 0.4503
Processing 11/40: Blurry
  Data split complete. Training Logistic Regression model for 'Blurry'...
  Completed Blurry. Accuracy: 0.9524, Precision: 0.6818, Recall: 0.0230, F1-Score: 0.0446
Processing 12/40: Brown_Hair
  Data split complete. Training Logistic Regression model for 'Brown_Hair'...
  Completed Brown_Hair. Accuracy: 0.8048, Precision: 0.5628, Recall: 0.2938, F1-Score: 0.3861
Processing 13/40: Bushy_Eyebrows
  Data split complete. Training Logistic Regression model for 'Bushy_Eyebrows'...
  Completed Bushy_Eyebrows. Accuracy: 0.8719, Precision: 0.6365, Recall: 0.2353, F1-Score: 0.3436
Processing 14/40: Chubby
  Data split complete. Training Logistic Regression model for 'Chubby'...
  Completed Chubby. Accuracy: 0.9558, Precision: 0.6873, Recall: 0.4234, F1-Score: 0.5240
Processing 15/40: Double_Chin
  Data split complete. Training Logistic Regression model for 'Double_Chin'...
  Completed Double_Chin. Accuracy: 0.9642, Precision: 0.6840, Recall: 0.4364, F1-Score: 0.5329
Processing 16/40: Eyeglasses
  Data split complete. Training Logistic Regression model for 'Eyeglasses'...
  Completed Eyeglasses. Accuracy: 0.9373, Precision: 0.5446, Recall: 0.1155, F1-Score: 0.1906
Processing 17/40: Goatee
  Data split complete. Training Logistic Regression model for 'Goatee'...
  Completed Goatee. Accuracy: 0.9518, Precision: 0.6408, Recall: 0.5241, F1-Score: 0.5766
Processing 18/40: Gray_Hair
  Data split complete. Training Logistic Regression model for 'Gray_Hair'...
  Completed Gray_Hair. Accuracy: 0.9626, Precision: 0.6080, Recall: 0.3263, F1-Score: 0.4247
Processing 19/40: Heavy_Makeup
  Data split complete. Training Logistic Regression model for 'Heavy_Makeup'...
  Completed Heavy_Makeup. Accuracy: 0.9063, Precision: 0.8400, Recall: 0.9361, F1-Score: 0.8855
Processing 20/40: High_Cheekbones
  Data split complete. Training Logistic Regression model for 'High_Cheekbones'...
  Completed High_Cheekbones. Accuracy: 0.8508, Precision: 0.8311, Recall: 0.8430, F1-Score: 0.8370
Processing 21/40: Male
  Data split complete. Training Logistic Regression model for 'Male'...
  Completed Male. Accuracy: 0.9323, Precision: 0.8955, Recall: 0.9479, F1-Score: 0.9209
Processing 22/40: Mouth_Slightly_Open
  Data split complete. Training Logistic Regression model for 'Mouth_Slightly_Open'...
  Completed Mouth_Slightly_Open. Accuracy: 0.7633, Precision: 0.7601, Recall: 0.7526, F1-Score: 0.7563
Processing 23/40: Mustache
  Data split complete. Training Logistic Regression model for 'Mustache'...
  Completed Mustache. Accuracy: 0.9639, Precision: 0.6205, Recall: 0.3195, F1-Score: 0.4218
Processing 24/40: Narrow_Eyes
  Data split complete. Training Logistic Regression model for 'Narrow_Eyes'...
  Completed Narrow_Eyes. Accuracy: 0.8848, Precision: 0.5538, Recall: 0.0077, F1-Score: 0.0152
Processing 25/40: No_Beard
  Data split complete. Training Logistic Regression model for 'No_Beard'...
  Completed No_Beard. Accuracy: 0.9452, Precision: 0.9655, Recall: 0.9690, F1-Score: 0.9673
Processing 26/40: Oval_Face
  Data split complete. Training Logistic Regression model for 'Oval_Face'...
  Completed Oval_Face. Accuracy: 0.7535, Precision: 0.6342, Recall: 0.3393, F1-Score: 0.4421
Processing 27/40: Pale_Skin
  Data split complete. Training Logistic Regression model for 'Pale_Skin'...
  Completed Pale_Skin. Accuracy: 0.9581, Precision: 1.0000, Recall: 0.0000, F1-Score: 0.0000
Processing 28/40: Pointy_Nose
  Data split complete. Training Logistic Regression model for 'Pointy_Nose'...
  Completed Pointy_Nose. Accuracy: 0.7344, Precision: 0.5779, Recall: 0.1794, F1-Score: 0.2738
Processing 29/40: Receding_Hairline
  Data split complete. Training Logistic Regression model for 'Receding_Hairline'...
  Completed Receding_Hairline. Accuracy: 0.9230, Precision: 0.5646, Recall: 0.1318, F1-Score: 0.2137
Processing 30/40: Rosy_Cheeks
  Data split complete. Training Logistic Regression model for 'Rosy_Cheeks'...
  Completed Rosy_Cheeks. Accuracy: 0.9355, Precision: 0.5728, Recall: 0.1337, F1-Score: 0.2168
Processing 31/40: Sideburns
  Data split complete. Training Logistic Regression model for 'Sideburns'...
  Completed Sideburns. Accuracy: 0.9555, Precision: 0.6561, Recall: 0.4359, F1-Score: 0.5238
Processing 32/40: Smiling
  Data split complete. Training Logistic Regression model for 'Smiling'...
  Completed Smiling. Accuracy: 0.8526, Precision: 0.8566, Recall: 0.8346, F1-Score: 0.8454
Processing 33/40: Straight_Hair
  Data split complete. Training Logistic Regression model for 'Straight_Hair'...
  Completed Straight_Hair. Accuracy: 0.7948, Precision: 0.5342, Recall: 0.0946, F1-Score: 0.1607
Processing 34/40: Wavy_Hair
  Data split complete. Training Logistic Regression model for 'Wavy_Hair'...
  Completed Wavy_Hair. Accuracy: 0.7663, Precision: 0.6356, Recall: 0.6317, F1-Score: 0.6336
Processing 35/40: Wearing_Earrings
  Data split complete. Training Logistic Regression model for 'Wearing_Earrings'...
  Completed Wearing_Earrings. Accuracy: 0.8295, Precision: 0.6239, Recall: 0.2767, F1-Score: 0.3834
Processing 36/40: Wearing_Hat
  Data split complete. Training Logistic Regression model for 'Wearing_Hat'...
  Completed Wearing_Hat. Accuracy: 0.9522, Precision: 0.6090, Recall: 0.0812, F1-Score: 0.1432
Processing 37/40: Wearing_Lipstick
  Data split complete. Training Logistic Regression model for 'Wearing_Lipstick'...
  Completed Wearing_Lipstick. Accuracy: 0.9295, Precision: 0.9224, Recall: 0.9290, F1-Score: 0.9257
Processing 38/40: Wearing_Necklace
  Data split complete. Training Logistic Regression model for 'Wearing_Necklace'...
  Completed Wearing_Necklace. Accuracy: 0.8798, Precision: 0.5626, Recall: 0.0681, F1-Score: 0.1215
Processing 39/40: Wearing_Necktie
  Data split complete. Training Logistic Regression model for 'Wearing_Necktie'...
  Completed Wearing_Necktie. Accuracy: 0.9301, Precision: 0.5950, Recall: 0.1294, F1-Score: 0.2125
Processing 40/40: Young
  Data split complete. Training Logistic Regression model for 'Young'...
  Completed Young. Accuracy: 0.8496, Precision: 0.8612, Recall: 0.9599, F1-Score: 0.9079

Logistic Regression Results:
              Attribute  Accuracy  Precision    Recall  F1-Score
0      5_o_Clock_Shadow  0.925962   0.723488  0.532543  0.613502
1       Arched_Eyebrows  0.802443   0.666038  0.522085  0.585341
2            Attractive  0.786229   0.778317  0.813206  0.795379
3       Bags_Under_Eyes  0.832651   0.657713  0.397470  0.495499
4                  Bald  0.978060   0.548544  0.124312  0.202691
5                 Bangs  0.847137   0.617978  0.026370  0.050582
6              Big_Lips  0.774013   0.587991  0.216992  0.316999
7              Big_Nose  0.832848   0.704993  0.501934  0.586382
8            Black_Hair  0.800740   0.625139  0.407711  0.493539
9            Blond_Hair  0.867399   0.586620  0.365433  0.450332
10               Blurry  0.952394   0.681818  0.023041  0.044577
11           Brown_Hair  0.804788   0.562797  0.293798  0.386060
12       Bushy_Eyebrows  0.871866   0.636534  0.235325  0.343616
13               Chubby  0.955750   0.687326  0.423423  0.524024
14          Double_Chin  0.964215   0.684036  0.436412  0.532861
15           Eyeglasses  0.937340   0.544627  0.115533  0.190628
16               Goatee  0.951826   0.640791  0.524054  0.576573
17            Gray_Hair  0.962562   0.608035  0.326340  0.424725
18         Heavy_Makeup  0.906269   0.840046  0.936121  0.885485
19      High_Cheekbones  0.850790   0.831138  0.842963  0.837009
20                 Male  0.932330   0.895451  0.947872  0.920916
21  Mouth_Slightly_Open  0.763327   0.760125  0.752591  0.756339
22             Mustache  0.963944   0.620489  0.319544  0.421844
23          Narrow_Eyes  0.884822   0.553846  0.007702  0.015193
24             No_Beard  0.945188   0.965535  0.969041  0.967285
25            Oval_Face  0.753504   0.634236  0.339278  0.442074
26            Pale_Skin  0.958095   1.000000  0.000000  0.000000
27          Pointy_Nose  0.734403   0.577898  0.179415  0.273819
28    Receding_Hairline  0.923001   0.564581  0.131800  0.213710
29          Rosy_Cheeks  0.935464   0.572785  0.133727  0.216831
30            Sideburns  0.955479   0.656085  0.435852  0.523759
31              Smiling  0.852616   0.856551  0.834577  0.845421
32        Straight_Hair  0.794842   0.534228  0.094593  0.160727
33            Wavy_Hair  0.766264   0.635623  0.631652  0.633631
34     Wearing_Earrings  0.829492   0.623875  0.276733  0.383400
35          Wearing_Hat  0.952172   0.609023  0.081162  0.143236
36     Wearing_Lipstick  0.929492   0.922419  0.929015  0.925705
37     Wearing_Necklace  0.879763   0.562604  0.068122  0.121529
38      Wearing_Necktie  0.930133   0.595016  0.129360  0.212517
39                Young  0.849556   0.861239  0.959867  0.907882

Conditional Probability Table:
                    5_o_Clock_Shadow Arched_Eyebrows Attractive  \
5_o_Clock_Shadow                 1.0        0.028545   0.092004   
Arched_Eyebrows             0.068573             1.0   0.375102   
Attractive                  0.424276        0.720059        1.0   
Bags_Under_Eyes             0.396785        0.143113    0.13436   
Bald                        0.024782         0.00538   0.001377   
Bangs                       0.061068        0.135071    0.17246   
Big_Lips                    0.187955        0.413237    0.26688   
Big_Nose                    0.416681        0.175541   0.120039   
Black_Hair                  0.360632        0.238547   0.240973   
Blond_Hair                  0.014745        0.222721   0.201593   
Blurry                      0.030156        0.023239   0.012106   
Brown_Hair                  0.191153        0.217138   0.257211   
Bushy_Eyebrows              0.357435        0.131725   0.156569   
Chubby                      0.050808        0.023054   0.003756   
Double_Chin                 0.048366        0.019375   0.003689   
Eyeglasses                  0.070217        0.004234   0.011528   
Goatee                      0.161441        0.017434   0.028084   
Gray_Hair                   0.017099        0.009004   0.002494   
Heavy_Makeup                0.000444        0.741727   0.613543   
High_Cheekbones             0.228948        0.583953   0.527443   
Male                        0.999112        0.083435   0.227086   
Mouth_Slightly_Open         0.389456        0.538843   0.493889   
Mustache                    0.093533        0.012997   0.014263   
Narrow_Eyes                 0.125333        0.127935   0.092148   
No_Beard                    0.281711        0.959641   0.906504   
Oval_Face                   0.175298        0.274099   0.369449   
Pale_Skin                   0.019853        0.058976   0.059962   
Pointy_Nose                 0.246669        0.390996   0.377134   
Receding_Hairline           0.062711         0.07225   0.032562   
Rosy_Cheeks                 0.002576        0.157867   0.105323   
Sideburns                   0.225884        0.012128   0.033939   
Smiling                     0.386303        0.559771    0.55413   
Straight_Hair               0.268431        0.171732   0.224861   
Wavy_Hair                   0.155711        0.474746   0.417343   
Wearing_Earrings            0.009682        0.380126   0.236399   
Wearing_Hat                 0.069106        0.012886   0.019425   
Wearing_Lipstick            0.000977          0.8533   0.706201   
Wearing_Necklace            0.015012        0.242781   0.144983   
Wearing_Necktie             0.144608        0.015567   0.033101   
Young                       0.791215        0.875411   0.931871   

                    Bags_Under_Eyes      Bald     Bangs  Big_Lips  Big_Nose  \
5_o_Clock_Shadow           0.215558  0.122718  0.044775  0.086748  0.197449   
Arched_Eyebrows            0.186773  0.063998  0.237911  0.458174  0.199827   
Attractive                 0.336607  0.031449  0.583119  0.568023  0.262312   
Bags_Under_Eyes                 1.0  0.513965   0.14963  0.200287  0.468095   
Bald                       0.056387       1.0       0.0  0.021503   0.07065   
Bangs                      0.110867       0.0       1.0  0.173783  0.106617   
Big_Lips                   0.235753  0.230702  0.276075       1.0   0.30013   
Big_Nose                    0.53665  0.738289  0.164968  0.292323       1.0   
Black_Hair                 0.241495  0.012976   0.20538  0.290151  0.301751   
Blond_Hair                 0.073155   0.00022   0.23104  0.162919  0.046426   
Blurry                     0.036698  0.037387  0.046175  0.036425  0.035925   
Brown_Hair                 0.168171   0.00066  0.271549  0.194179  0.108006   
Bushy_Eyebrows             0.215895  0.095667  0.082223  0.155068  0.231227   
Chubby                     0.129807  0.401364  0.011202  0.060797  0.189831   
Double_Chin                0.128312  0.343083  0.011983  0.043108   0.16043   
Eyeglasses                 0.044492  0.243237  0.030675  0.042985  0.127326   
Goatee                     0.108406  0.247196  0.013644  0.070985  0.147845   
Gray_Hair                  0.109806  0.242138  0.012732  0.010557  0.112846   
Heavy_Makeup               0.105221   0.00044  0.525677  0.513662  0.141889   
High_Cheekbones            0.526589  0.447548  0.517438  0.494968  0.507892   
Male                       0.709453  0.996261  0.226318  0.270145  0.745665   
Mouth_Slightly_Open        0.541331  0.481856  0.494643  0.526883  0.537061   
Mustache                   0.084809   0.14647  0.009704  0.052701  0.116487   
Narrow_Eyes                0.182406  0.141192   0.12498  0.179522  0.155611   
No_Beard                    0.72767  0.551572  0.951252  0.849954  0.664723   
Oval_Face                  0.163273  0.316252  0.285877   0.19424  0.197849   
Pale_Skin                  0.029798  0.011876  0.063174  0.057108  0.024202   
Pointy_Nose                0.176808  0.113042  0.289068  0.319299  0.149129   
Receding_Hairline          0.142306  0.330548   0.00013  0.089864  0.178866   
Rosy_Cheeks                0.019809  0.004618  0.101469  0.099498  0.040955   
Sideburns                  0.102229   0.14581  0.016445  0.040217  0.111436   
Smiling                    0.593181  0.513086  0.544108  0.493512  0.573154   
Straight_Hair               0.22779  0.015615  0.227653  0.181613  0.187242   
Wavy_Hair                  0.202673    0.0011   0.39477  0.419084  0.208498   
Wearing_Earrings           0.114076   0.03079  0.242828  0.276868  0.147887   
Wearing_Hat                0.046036  0.005058  0.007848  0.043394  0.073891   
Wearing_Lipstick           0.192805  0.001759  0.665408  0.645485  0.198565   
Wearing_Necklace            0.08867  0.013635  0.211892  0.210372   0.10165   
Wearing_Necktie            0.173575  0.375192  0.015631  0.041468   0.16889   
Young                      0.583193  0.231581  0.791136  0.853664  0.558612   

                    Black_Hair Blond_Hair  ... Sideburns   Smiling  \
5_o_Clock_Shadow      0.167519   0.011073  ...  0.444231  0.089056   
Arched_Eyebrows       0.266195   0.401794  ...  0.057298  0.310006   
Attractive            0.516195   0.698129  ...    0.3078  0.589102   
Bags_Under_Eyes        0.20649   0.101124  ...  0.370076  0.251718   
Bald                  0.001217   0.000033  ...  0.057909  0.023887   
Bangs                 0.130116   0.236634  ...  0.044109  0.171078   
Big_Lips              0.292024   0.265084  ...  0.171369  0.246506   
Big_Nose                0.2958   0.073575  ...  0.462486   0.27884   
Black_Hair                 1.0     0.0001  ...  0.314613  0.238131   
Blond_Hair            0.000062        1.0  ...  0.008647  0.181194   
Blurry                0.036021   0.045359  ...  0.028736  0.038712   
Brown_Hair            0.023436    0.04069  ...  0.147873  0.214991   
Bushy_Eyebrows        0.302814   0.016376  ...  0.325443  0.141109   
Chubby                0.061623   0.007371  ...    0.1677  0.066265   
Double_Chin           0.036248   0.007838  ...  0.070749  0.068579   
Eyeglasses            0.058013   0.017243  ...  0.112499  0.054285   
Goatee                0.088278   0.003102  ...  0.571054  0.043862   
Gray_Hair             0.000206   0.017076  ...   0.04734  0.043975   
Heavy_Makeup          0.344157   0.675149  ...  0.000175  0.476088   
High_Cheekbones       0.462576   0.599373  ...  0.180802   0.80782   
Male                   0.51898   0.058333  ...  0.999039  0.346046   
Mouth_Slightly_Open   0.462783   0.567488  ...  0.330335  0.761255   
Mustache              0.064264     0.0005  ...  0.315748  0.027634   
Narrow_Eyes           0.108888   0.114065  ...  0.114945  0.141273   
No_Beard               0.76962   0.986959  ...   0.01118  0.878334   
Oval_Face             0.309787   0.338192  ...  0.193467  0.380356   
Pale_Skin             0.028965    0.07124  ...  0.010219  0.028105   
Pointy_Nose           0.240221   0.401461  ...  0.186217   0.29728   
Receding_Hairline     0.079778   0.033986  ...  0.102018  0.087192   
Rosy_Cheeks           0.048131   0.149885  ...  0.002096  0.122485   
Sideburns             0.074311   0.003302  ...       1.0  0.037228   
Smiling               0.479823   0.590234  ...  0.317582       1.0   
Straight_Hair         0.289879   0.213721  ...  0.175474  0.210773   
Wavy_Hair               0.2487   0.463229  ...  0.181763  0.355824   
Wearing_Earrings      0.190564   0.278158  ...  0.009346  0.258004   
Wearing_Hat           0.008789   0.005503  ...  0.107695  0.034218   
Wearing_Lipstick      0.412919   0.810159  ...  0.000262  0.566423   
Wearing_Necklace      0.098057     0.2361  ...  0.014848  0.153263   
Wearing_Necktie        0.08345   0.006437  ...  0.136781  0.072111   
Young                  0.86419   0.826235  ...  0.619879  0.759217   

                    Straight_Hair Wavy_Hair Wearing_Earrings Wearing_Hat  \
5_o_Clock_Shadow         0.143148  0.054152         0.005695    0.158484   
Arched_Eyebrows          0.220004  0.396624         0.537177    0.070992   
Attractive               0.552982  0.669313          0.64129    0.205439   
Bags_Under_Eyes          0.223604  0.129742         0.123524    0.194337   
Bald                     0.001682  0.000077         0.003658    0.002343   
Bangs                    0.165577  0.187245         0.194822    0.024547   
Big_Lips                 0.209843  0.315782         0.352884    0.215624   
Big_Nose                  0.21072  0.153018         0.183588    0.357608   
Black_Hair               0.332789  0.186195         0.241326     0.04339   
Blond_Hair               0.151769  0.214522         0.217891    0.016806   
Blurry                   0.035171  0.044637         0.024532    0.066816   
Brown_Hair                0.19253  0.295827         0.206343    0.029028   
Bushy_Eyebrows           0.192554  0.110759         0.087026     0.11153   
Chubby                   0.042513  0.024574          0.03062    0.118456   
Double_Chin              0.035455  0.021114         0.023357    0.075983   
Eyeglasses               0.056558  0.030875         0.023435    0.142697   
Goatee                   0.042584  0.025655          0.01126    0.156244   
Gray_Hair                0.037871  0.016944         0.019229    0.009676   
Heavy_Makeup             0.322604   0.61669         0.743259    0.083316   
High_Cheekbones           0.43681  0.539031          0.69686    0.264005   
Male                     0.484842  0.183677         0.035244    0.699837   
Mouth_Slightly_Open      0.469495  0.512603         0.618273    0.491139   
Mustache                 0.031382  0.017098         0.008726     0.11153   
Narrow_Eyes              0.117451  0.125541         0.120467    0.097983   
No_Beard                 0.850883  0.921784          0.97808    0.638317   
Oval_Face                0.287836  0.309218         0.354609    0.191892   
Pale_Skin                0.049192  0.049626         0.034957    0.028315   
Pointy_Nose              0.262754   0.36649         0.376999    0.124771   
Receding_Hairline        0.049713  0.034243          0.08666    0.001732   
Rosy_Cheeks              0.051963  0.112953         0.176272    0.009371   
Sideburns                0.047582  0.032142         0.002795    0.125586   
Smiling                  0.487566  0.536776          0.65835    0.340395   
Straight_Hair                 1.0  0.017886         0.144555    0.020676   
Wavy_Hair                0.027426       1.0         0.433065    0.068446   
Wearing_Earrings         0.131045  0.256024              1.0    0.099613   
Wearing_Hat              0.004808  0.010379         0.025551         1.0   
Wearing_Lipstick         0.413836  0.734184         0.851656    0.118863   
Wearing_Necklace         0.102719  0.184959         0.254311    0.065798   
Wearing_Necktie          0.112548  0.018766         0.003344    0.039214   
Young                    0.816967  0.828478         0.799718    0.706559   

                    Wearing_Lipstick Wearing_Necklace Wearing_Necktie  \
5_o_Clock_Shadow             0.00023         0.013567        0.221015   
Arched_Eyebrows             0.482213         0.527114        0.057154   
Attractive                  0.766097         0.604263        0.233302   
Bags_Under_Eyes             0.083487         0.147513        0.488325   
Bald                        0.000084         0.002489        0.115802   
Bangs                       0.213488         0.261189        0.032582   
Big_Lips                    0.328998         0.411954         0.13732   
Big_Nose                    0.098574         0.193875        0.544733   
Black_Hair                   0.20911         0.190784        0.274572   
Blond_Hair                  0.253785         0.284149        0.013101   
Blurry                      0.021313         0.048328        0.039166   
Brown_Hair                  0.248143         0.200618        0.100326   
Bushy_Eyebrows              0.078796         0.075824        0.221423   
Chubby                      0.009936         0.027215        0.220337   
Double_Chin                 0.008807         0.024646        0.216332   
Eyeglasses                   0.01097         0.035122        0.180695   
Goatee                      0.000136         0.012323        0.121844   
Gray_Hair                   0.007836         0.019869        0.216603   
Heavy_Makeup                0.799457         0.651748        0.001018   
High_Cheekbones             0.604628         0.619155        0.372115   
Male                        0.005464         0.060491        0.997624   
Mouth_Slightly_Open         0.538536         0.589893        0.440334   
Mustache                    0.000063         0.010878        0.114852   
Narrow_Eyes                 0.106671         0.141573        0.126731   
No_Beard                    0.999122         0.973347        0.679609   
Oval_Face                   0.359139         0.215149        0.199633   
Pale_Skin                   0.056867          0.04315        0.021993   
Pointy_Nose                 0.398224         0.354915        0.183342   
Receding_Hairline           0.044141         0.053346          0.2295   
Rosy_Cheeks                 0.135465         0.157187        0.006584   
Sideburns                   0.000031         0.006824        0.106299   
Smiling                     0.577987         0.600851        0.478075   
Straight_Hair               0.182552         0.174086        0.322563   
Wavy_Hair                    0.49662         0.480673        0.082474   
Wearing_Earrings            0.340574          0.39072        0.008689   
Wearing_Hat                 0.012192          0.02593        0.026134   
Wearing_Lipstick                 1.0         0.826878        0.001969   
Wearing_Necklace            0.215222              1.0        0.000543   
Wearing_Necktie             0.000303         0.000321             1.0   
Young                       0.884981         0.790029        0.396416   

                        Young  
5_o_Clock_Shadow     0.113664  
Arched_Eyebrows      0.302111  
Attractive           0.617345  
Bags_Under_Eyes      0.154217  
Bald                 0.006718  
Bangs                0.155008  
Big_Lips             0.265711  
Big_Nose             0.169351  
Black_Hair           0.267262  
Blond_Hair           0.158058  
Blurry               0.043016  
Brown_Hair            0.22764  
Bushy_Eyebrows       0.158262  
Chubby                0.02043  
Double_Chin          0.011331  
Eyeglasses           0.035117  
Goatee                0.04879  
Gray_Hair            0.002424  
Heavy_Makeup         0.451076  
High_Cheekbones      0.450547  
Male                 0.341005  
Mouth_Slightly_Open  0.479577  
Mustache             0.026746  
Narrow_Eyes          0.109396  
No_Beard             0.858761  
Oval_Face            0.311643  
Pale_Skin            0.047718  
Pointy_Nose          0.299495  
Receding_Hairline    0.051699  
Rosy_Cheeks          0.071446  
Sideburns            0.045281  
Smiling              0.473107  
Straight_Hair         0.22008  
Wavy_Hair             0.34223  
Wearing_Earrings     0.195299  
Wearing_Hat           0.04426  
Wearing_Lipstick     0.540444  
Wearing_Necklace     0.125576  
Wearing_Necktie      0.037261  
Young                     1.0  

[40 rows x 40 columns]</code></pre>
</div>
</div>
<p>This is an interesting probability table to dig through and see the associations between various different attributes in the dataset.</p>
<p>For example, Smiling and Mouth Slightly Open had a relatively high conditional probability, meaning these attributes often occur together.</p>
<p>Conversely, attributes like Bald and Young had very low conditional probabilities, reflecting the mutual exclusivity of these features. Similarly, Heavy Makeup and Wearing Lipstick suggest a strong association between these features.</p>
<p>In this next phase of the project, we applied clustering techniques to understand patterns in the dataset using MiniBatchKMeans. We also used PCA (Principal Component Analysis) to reduce the dimensions of the data for visualisation.</p>
<p>We loaded the attribute and partition data, ensuring that attributes like ‘-1’ (indicating absence of a feature) were converted to ‘0’. The two datasets were merged based on image_id, and unnecessary columns like image_id and partition were removed, leaving only the binary attributes.</p>
<p>We sampled 10% of the data to make the clustering task more computationally efficient. We scaled the attributes using StandardScaler to ensure that all features contributed equally to the clustering process.</p>
<p>We employed MiniBatchKMeans (a faster version of KMeans for large datasets) to cluster the sampled dataset into 5 clusters.</p>
<p>The model identified patterns in the facial attributes by grouping them into clusters based on their similarity.</p>
<p>To visualize the high-dimensional data, we applied PCA to reduce the dimensions of the dataset to 2 components.</p>
<p>This allowed us to visualize the clusters formed by MiniBatchKMeans on a 2D plane, helping us observe how well the attributes were</p>
<div id="801c9a06" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> MiniBatchKMeans</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the paths</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>attr_path <span class="op">=</span> <span class="st">'/Users/johnaziz/Downloads/archive-3/list_attr_celeba.csv'</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>partition_path <span class="op">=</span> <span class="st">'/Users/johnaziz/Downloads/archive-3/list_eval_partition.csv'</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the attribute data</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loading attribute data..."</span>)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>attributes_df <span class="op">=</span> pd.read_csv(attr_path)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>attributes_df.columns <span class="op">=</span> attributes_df.columns.<span class="bu">str</span>.strip()  <span class="co"># Remove any leading/trailing whitespace from column names</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert -1 to 0 for binary classification (0 = attribute absent, 1 = attribute present)</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>attributes_df.iloc[:, <span class="dv">1</span>:] <span class="op">=</span> attributes_df.iloc[:, <span class="dv">1</span>:].replace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the partition data</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loading partition data..."</span>)</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>partition_df <span class="op">=</span> pd.read_csv(partition_path)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>partition_df.columns <span class="op">=</span> partition_df.columns.<span class="bu">str</span>.strip()  <span class="co"># Remove any leading/trailing whitespace from column names</span></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Merge the two datasets on the image_id</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Merging attributes and partition data..."</span>)</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> pd.merge(attributes_df, partition_df, on<span class="op">=</span><span class="st">"image_id"</span>)</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop unnecessary columns like 'image_id' and 'partition'</span></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>attributes_only <span class="op">=</span> train_df.drop(columns<span class="op">=</span>[<span class="st">'image_id'</span>, <span class="st">'partition'</span>])</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample a subset of the data (e.g., 10% of the dataset) for testing</span></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>attributes_sample <span class="op">=</span> attributes_only.sample(frac<span class="op">=</span><span class="fl">0.1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale the attributes</span></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>attributes_scaled <span class="op">=</span> scaler.fit_transform(attributes_sample)</span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Verbose print to monitor progress</span></span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Starting MiniBatchKMeans clustering on a 10</span><span class="sc">% s</span><span class="st">ample..."</span>)</span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Track the time taken for MiniBatchKMeans</span></span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a><span class="co"># MiniBatchKMeans Clustering</span></span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>minibatch_kmeans <span class="op">=</span> MiniBatchKMeans(n_clusters<span class="op">=</span><span class="dv">5</span>, random_state<span class="op">=</span><span class="dv">42</span>, batch_size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>minibatch_kmeans.fit(attributes_scaled)</span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the cluster labels to the DataFrame</span></span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a>attributes_sample <span class="op">=</span> pd.DataFrame(attributes_scaled, columns<span class="op">=</span>attributes_only.columns)</span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a>attributes_sample[<span class="st">'Cluster'</span>] <span class="op">=</span> minibatch_kmeans.labels_</span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Print completion time for MiniBatchKMeans</span></span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a>end_time <span class="op">=</span> time.time()</span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MiniBatchKMeans clustering completed in </span><span class="sc">{</span>end_time <span class="op">-</span> start_time<span class="sc">:.2f}</span><span class="ss"> seconds."</span>)</span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the size of each cluster</span></span>
<span id="cb17-59"><a href="#cb17-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Cluster sizes:"</span>)</span>
<span id="cb17-60"><a href="#cb17-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attributes_sample[<span class="st">'Cluster'</span>].value_counts())</span>
<span id="cb17-61"><a href="#cb17-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-62"><a href="#cb17-62" aria-hidden="true" tabindex="-1"></a><span class="co"># Verbose print for PCA</span></span>
<span id="cb17-63"><a href="#cb17-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Starting PCA for dimensionality reduction..."</span>)</span>
<span id="cb17-64"><a href="#cb17-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-65"><a href="#cb17-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Track the time taken for PCA</span></span>
<span id="cb17-66"><a href="#cb17-66" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb17-67"><a href="#cb17-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-68"><a href="#cb17-68" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the clusters using PCA (reduce dimensions to 2 for visualization)</span></span>
<span id="cb17-69"><a href="#cb17-69" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb17-70"><a href="#cb17-70" aria-hidden="true" tabindex="-1"></a>pca_result <span class="op">=</span> pca.fit_transform(attributes_scaled)</span>
<span id="cb17-71"><a href="#cb17-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-72"><a href="#cb17-72" aria-hidden="true" tabindex="-1"></a><span class="co"># Print completion time for PCA</span></span>
<span id="cb17-73"><a href="#cb17-73" aria-hidden="true" tabindex="-1"></a>end_time <span class="op">=</span> time.time()</span>
<span id="cb17-74"><a href="#cb17-74" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PCA completed in </span><span class="sc">{</span>end_time <span class="op">-</span> start_time<span class="sc">:.2f}</span><span class="ss"> seconds."</span>)</span>
<span id="cb17-75"><a href="#cb17-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-76"><a href="#cb17-76" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the PCA result with cluster labels</span></span>
<span id="cb17-77"><a href="#cb17-77" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">7</span>))</span>
<span id="cb17-78"><a href="#cb17-78" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span>pca_result[:, <span class="dv">0</span>], y<span class="op">=</span>pca_result[:, <span class="dv">1</span>], hue<span class="op">=</span>attributes_sample[<span class="st">'Cluster'</span>], palette<span class="op">=</span><span class="st">'viridis'</span>, s<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb17-79"><a href="#cb17-79" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'MiniBatchKMeans Clusters of CelebA Attributes (Sampled Data)'</span>)</span>
<span id="cb17-80"><a href="#cb17-80" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Principal Component 1'</span>)</span>
<span id="cb17-81"><a href="#cb17-81" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Principal Component 2'</span>)</span>
<span id="cb17-82"><a href="#cb17-82" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-83"><a href="#cb17-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-84"><a href="#cb17-84" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Plotting complete."</span>)</span>
<span id="cb17-85"><a href="#cb17-85" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loading attribute data...
Loading partition data...
Merging attributes and partition data...
Starting MiniBatchKMeans clustering on a 10% sample...</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/johnaziz/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1930: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=3)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>MiniBatchKMeans clustering completed in 0.11 seconds.
Cluster sizes:
Cluster
3    6578
1    5723
4    4735
2    1791
0    1433
Name: count, dtype: int64
Starting PCA for dimensionality reduction...
PCA completed in 0.07 seconds.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Computer%20Vision%20Faces_files/figure-html/cell-6-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Plotting complete.</code></pre>
</div>
</div>
<p>The MiniBatchKMeans identified 5 distinct clusters, and we plotted the clusters using a scatterplot, with colors representing different clusters. This gave us a visual insight into how the attributes were grouped.</p>
<p>Next, we sought to etend the clustering analysis of the attributes by adding additional steps for visualizing, validating, and exploring the clusters.</p>
<p>We calculate the average attributes for each cluster, to generate insights into the feature distribution across clusters.</p>
<p>We produced an elbow plot to help assess whether 5 clusters is the optimal number.</p>
<p>We produced t-SNE plot shows the clusters in a reduced 2D space, providing a visual validation of cluster separation.</p>
<p>Next, we produced a system for displaying sample images from each cluster to help give a more visual and real-world sense of the groupings formed during clustering.</p>
<p>Finally, we produced an inter-cluster Distance Heatmap: to highlight how different or similar the clusters are from one another based on their centroids.</p>
<div id="e96d3412-3cbf-408b-b69e-f9ec4728388e" class="cell" data-tags="[]" data-execution_count="47">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> MiniBatchKMeans</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial.distance <span class="im">import</span> cdist</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Paths</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>attr_path <span class="op">=</span> <span class="st">'/Users/johnaziz/Downloads/archive-3/list_attr_celeba.csv'</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>partition_path <span class="op">=</span> <span class="st">'/Users/johnaziz/Downloads/archive-3/list_eval_partition.csv'</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>image_dir <span class="op">=</span> <span class="st">'/Users/johnaziz/Downloads/archive-3/img_align_celeba/img_align_celeba/'</span>  </span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the attribute and partition data</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loading attribute data..."</span>)</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>attributes_df <span class="op">=</span> pd.read_csv(attr_path)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>attributes_df.columns <span class="op">=</span> attributes_df.columns.<span class="bu">str</span>.strip()</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loading partition data..."</span>)</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>partition_df <span class="op">=</span> pd.read_csv(partition_path)</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>partition_df.columns <span class="op">=</span> partition_df.columns.<span class="bu">str</span>.strip()</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert -1 to 0 for binary classification</span></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>attributes_df.iloc[:, <span class="dv">1</span>:] <span class="op">=</span> attributes_df.iloc[:, <span class="dv">1</span>:].replace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Merge attributes and partition data</span></span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Merging attributes and partition data..."</span>)</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> pd.merge(attributes_df, partition_df, on<span class="op">=</span><span class="st">"image_id"</span>)</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop unnecessary columns</span></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>attributes_only <span class="op">=</span> train_df.drop(columns<span class="op">=</span>[<span class="st">'image_id'</span>, <span class="st">'partition'</span>])</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample a subset of the data (e.g., 10% of the dataset) for testing</span></span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>attributes_sample <span class="op">=</span> attributes_only.sample(frac<span class="op">=</span><span class="fl">0.1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale the attributes</span></span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a>attributes_scaled <span class="op">=</span> scaler.fit_transform(attributes_sample)</span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Start MiniBatchKMeans clustering</span></span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Starting MiniBatchKMeans clustering..."</span>)</span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a>minibatch_kmeans <span class="op">=</span> MiniBatchKMeans(n_clusters<span class="op">=</span><span class="dv">5</span>, random_state<span class="op">=</span><span class="dv">42</span>, batch_size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a>minibatch_kmeans.fit(attributes_scaled)</span>
<span id="cb22-48"><a href="#cb22-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-49"><a href="#cb22-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the cluster labels to both attributes_sample and train_df (matching rows)</span></span>
<span id="cb22-50"><a href="#cb22-50" aria-hidden="true" tabindex="-1"></a>attributes_sample <span class="op">=</span> pd.DataFrame(attributes_scaled, columns<span class="op">=</span>attributes_only.columns)</span>
<span id="cb22-51"><a href="#cb22-51" aria-hidden="true" tabindex="-1"></a>attributes_sample[<span class="st">'Cluster'</span>] <span class="op">=</span> minibatch_kmeans.labels_</span>
<span id="cb22-52"><a href="#cb22-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-53"><a href="#cb22-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Now, add the 'Cluster' column back to the original train_df, matching the sampled rows</span></span>
<span id="cb22-54"><a href="#cb22-54" aria-hidden="true" tabindex="-1"></a>train_df.loc[attributes_sample.index, <span class="st">'Cluster'</span>] <span class="op">=</span> minibatch_kmeans.labels_</span>
<span id="cb22-55"><a href="#cb22-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-56"><a href="#cb22-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the size of each cluster</span></span>
<span id="cb22-57"><a href="#cb22-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Cluster sizes:"</span>)</span>
<span id="cb22-58"><a href="#cb22-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attributes_sample[<span class="st">'Cluster'</span>].value_counts())</span>
<span id="cb22-59"><a href="#cb22-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-60"><a href="#cb22-60" aria-hidden="true" tabindex="-1"></a><span class="co">### 1. Analyze Cluster Characteristics</span></span>
<span id="cb22-61"><a href="#cb22-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Mean attribute values for each cluster:"</span>)</span>
<span id="cb22-62"><a href="#cb22-62" aria-hidden="true" tabindex="-1"></a>cluster_means <span class="op">=</span> attributes_sample.groupby(<span class="st">'Cluster'</span>).mean()</span>
<span id="cb22-63"><a href="#cb22-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(cluster_means)</span>
<span id="cb22-64"><a href="#cb22-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-65"><a href="#cb22-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot heatmap of the cluster means</span></span>
<span id="cb22-66"><a href="#cb22-66" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb22-67"><a href="#cb22-67" aria-hidden="true" tabindex="-1"></a>sns.heatmap(cluster_means.T, cmap<span class="op">=</span><span class="st">"coolwarm"</span>, annot<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-68"><a href="#cb22-68" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Heatmap of Attribute Means per Cluster'</span>)</span>
<span id="cb22-69"><a href="#cb22-69" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Cluster'</span>)</span>
<span id="cb22-70"><a href="#cb22-70" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Attribute'</span>)</span>
<span id="cb22-71"><a href="#cb22-71" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb22-72"><a href="#cb22-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-73"><a href="#cb22-73" aria-hidden="true" tabindex="-1"></a><span class="co">### 2. Elbow Method to Determine Optimal Number of Clusters</span></span>
<span id="cb22-74"><a href="#cb22-74" aria-hidden="true" tabindex="-1"></a>inertias <span class="op">=</span> []</span>
<span id="cb22-75"><a href="#cb22-75" aria-hidden="true" tabindex="-1"></a>k_values <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>)</span>
<span id="cb22-76"><a href="#cb22-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-77"><a href="#cb22-77" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> k_values:</span>
<span id="cb22-78"><a href="#cb22-78" aria-hidden="true" tabindex="-1"></a>    minibatch_kmeans <span class="op">=</span> MiniBatchKMeans(n_clusters<span class="op">=</span>k, random_state<span class="op">=</span><span class="dv">42</span>, batch_size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb22-79"><a href="#cb22-79" aria-hidden="true" tabindex="-1"></a>    minibatch_kmeans.fit(attributes_scaled)</span>
<span id="cb22-80"><a href="#cb22-80" aria-hidden="true" tabindex="-1"></a>    inertias.append(minibatch_kmeans.inertia_)</span>
<span id="cb22-81"><a href="#cb22-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-82"><a href="#cb22-82" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the elbow method</span></span>
<span id="cb22-83"><a href="#cb22-83" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb22-84"><a href="#cb22-84" aria-hidden="true" tabindex="-1"></a>plt.plot(k_values, inertias, <span class="st">'-o'</span>)</span>
<span id="cb22-85"><a href="#cb22-85" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Elbow Method to Determine Optimal Number of Clusters'</span>)</span>
<span id="cb22-86"><a href="#cb22-86" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of Clusters (k)'</span>)</span>
<span id="cb22-87"><a href="#cb22-87" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Inertia (Sum of Squared Distances)'</span>)</span>
<span id="cb22-88"><a href="#cb22-88" aria-hidden="true" tabindex="-1"></a>plt.xticks(k_values)</span>
<span id="cb22-89"><a href="#cb22-89" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb22-90"><a href="#cb22-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-91"><a href="#cb22-91" aria-hidden="true" tabindex="-1"></a><span class="co">### 3. t-SNE for Visualization</span></span>
<span id="cb22-92"><a href="#cb22-92" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Starting t-SNE for dimensionality reduction..."</span>)</span>
<span id="cb22-93"><a href="#cb22-93" aria-hidden="true" tabindex="-1"></a>tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb22-94"><a href="#cb22-94" aria-hidden="true" tabindex="-1"></a>tsne_result <span class="op">=</span> tsne.fit_transform(attributes_scaled)</span>
<span id="cb22-95"><a href="#cb22-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-96"><a href="#cb22-96" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the t-SNE result with cluster labels</span></span>
<span id="cb22-97"><a href="#cb22-97" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">7</span>))</span>
<span id="cb22-98"><a href="#cb22-98" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span>tsne_result[:, <span class="dv">0</span>], y<span class="op">=</span>tsne_result[:, <span class="dv">1</span>], hue<span class="op">=</span>attributes_sample[<span class="st">'Cluster'</span>], palette<span class="op">=</span><span class="st">'viridis'</span>, s<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb22-99"><a href="#cb22-99" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'t-SNE Clusters of CelebA Attributes (Sampled Data)'</span>)</span>
<span id="cb22-100"><a href="#cb22-100" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'t-SNE Component 1'</span>)</span>
<span id="cb22-101"><a href="#cb22-101" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'t-SNE Component 2'</span>)</span>
<span id="cb22-102"><a href="#cb22-102" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb22-103"><a href="#cb22-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-104"><a href="#cb22-104" aria-hidden="true" tabindex="-1"></a><span class="co">### 4. Display Sample Images from Each Cluster</span></span>
<span id="cb22-105"><a href="#cb22-105" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> display_cluster_images(cluster_num, num_images<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb22-106"><a href="#cb22-106" aria-hidden="true" tabindex="-1"></a>    cluster_data <span class="op">=</span> train_df[train_df[<span class="st">'Cluster'</span>] <span class="op">==</span> cluster_num]</span>
<span id="cb22-107"><a href="#cb22-107" aria-hidden="true" tabindex="-1"></a>    image_ids <span class="op">=</span> cluster_data[<span class="st">'image_id'</span>].sample(n<span class="op">=</span>num_images, random_state<span class="op">=</span><span class="dv">42</span>).tolist()</span>
<span id="cb22-108"><a href="#cb22-108" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-109"><a href="#cb22-109" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb22-110"><a href="#cb22-110" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, image_id <span class="kw">in</span> <span class="bu">enumerate</span>(image_ids):</span>
<span id="cb22-111"><a href="#cb22-111" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb22-112"><a href="#cb22-112" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> image_id.endswith(<span class="st">'.jpg'</span>):</span>
<span id="cb22-113"><a href="#cb22-113" aria-hidden="true" tabindex="-1"></a>            img_path <span class="op">=</span> os.path.join(image_dir, <span class="ss">f"</span><span class="sc">{</span>image_id<span class="sc">}</span><span class="ss">.jpg"</span>)</span>
<span id="cb22-114"><a href="#cb22-114" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb22-115"><a href="#cb22-115" aria-hidden="true" tabindex="-1"></a>            img_path <span class="op">=</span> os.path.join(image_dir, image_id)  </span>
<span id="cb22-116"><a href="#cb22-116" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> Image.<span class="bu">open</span>(img_path)</span>
<span id="cb22-117"><a href="#cb22-117" aria-hidden="true" tabindex="-1"></a>        plt.subplot(<span class="dv">1</span>, num_images, i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb22-118"><a href="#cb22-118" aria-hidden="true" tabindex="-1"></a>        plt.imshow(img)</span>
<span id="cb22-119"><a href="#cb22-119" aria-hidden="true" tabindex="-1"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb22-120"><a href="#cb22-120" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="ss">f'Cluster </span><span class="sc">{</span>cluster_num<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb22-121"><a href="#cb22-121" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb22-122"><a href="#cb22-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-123"><a href="#cb22-123" aria-hidden="true" tabindex="-1"></a><span class="co"># Display 5 sample images from each cluster</span></span>
<span id="cb22-124"><a href="#cb22-124" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cluster <span class="kw">in</span> attributes_sample[<span class="st">'Cluster'</span>].unique():</span>
<span id="cb22-125"><a href="#cb22-125" aria-hidden="true" tabindex="-1"></a>    display_cluster_images(cluster, num_images<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb22-126"><a href="#cb22-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-127"><a href="#cb22-127" aria-hidden="true" tabindex="-1"></a><span class="co">### 5. Inter-cluster Distance Heatmap</span></span>
<span id="cb22-128"><a href="#cb22-128" aria-hidden="true" tabindex="-1"></a>centroids <span class="op">=</span> minibatch_kmeans.cluster_centers_</span>
<span id="cb22-129"><a href="#cb22-129" aria-hidden="true" tabindex="-1"></a>distances <span class="op">=</span> cdist(centroids, centroids)</span>
<span id="cb22-130"><a href="#cb22-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-131"><a href="#cb22-131" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot heatmap of the inter-cluster distances</span></span>
<span id="cb22-132"><a href="#cb22-132" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb22-133"><a href="#cb22-133" aria-hidden="true" tabindex="-1"></a>sns.heatmap(distances, annot<span class="op">=</span><span class="va">True</span>, cmap<span class="op">=</span><span class="st">'coolwarm'</span>, fmt<span class="op">=</span><span class="st">".2f"</span>)</span>
<span id="cb22-134"><a href="#cb22-134" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Inter-cluster Distance Heatmap'</span>)</span>
<span id="cb22-135"><a href="#cb22-135" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Cluster'</span>)</span>
<span id="cb22-136"><a href="#cb22-136" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Cluster'</span>)</span>
<span id="cb22-137"><a href="#cb22-137" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loading attribute data...
Loading partition data...
Merging attributes and partition data...
Starting MiniBatchKMeans clustering...</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/johnaziz/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1930: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=3)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Cluster sizes:
Cluster
3    6578
1    5723
4    4735
2    1791
0    1433
Name: count, dtype: int64

Mean attribute values for each cluster:
         5_o_Clock_Shadow  Arched_Eyebrows  Attractive  Bags_Under_Eyes  \
Cluster                                                                   
0               -0.096651        -0.501052   -0.987884         0.868152   
1               -0.353430         0.518107    0.513912        -0.211347   
2                0.852991        -0.433202   -0.509239         0.341458   
3                0.351109        -0.496026   -0.386724         0.209468   
4               -0.353985         0.378375    0.407695        -0.427446   

             Bald     Bangs  Big_Lips  Big_Nose  Black_Hair  Blond_Hair  ...  \
Cluster                                                                  ...   
0        1.199428 -0.352821 -0.269672  1.181759   -0.364762   -0.358408  ...   
1       -0.149169  0.214955  0.137953 -0.228385   -0.076991    0.410953  ...   
2        0.252597 -0.308701  0.061158  0.654210    0.280353   -0.402915  ...   
3       -0.092912 -0.111544 -0.257996  0.101412    0.150809   -0.314270  ...   
4       -0.149169  0.118696  0.250158 -0.469945   -0.112104    0.200761  ...   

         Sideburns   Smiling  Straight_Hair  Wavy_Hair  Wearing_Earrings  \
Cluster                                                                    
0        -0.136335  0.161372      -0.039087  -0.481216         -0.354227   
1        -0.240641  0.946678      -0.070434   0.383172          0.649148   
2         2.375972 -0.335077      -0.095092  -0.363045         -0.409659   
3        -0.234626 -0.170862       0.139904  -0.365973         -0.401716   
4        -0.240641 -0.828939      -0.061430   0.328251          0.035632   

         Wearing_Hat  Wearing_Lipstick  Wearing_Necklace  Wearing_Necktie  \
Cluster                                                                     
0          -0.059743         -0.875502         -0.279066         1.340242   
1          -0.193574          0.964261          0.404673        -0.283932   
2           0.386841         -0.942611         -0.306015         0.213793   
3           0.163961         -0.926164         -0.297424         0.101707   
4          -0.122055          0.742693          0.124284        -0.284595   

            Young  
Cluster            
0       -1.666307  
1        0.181071  
2       -0.322223  
3        0.014282  
4        0.387476  

[5 rows x 40 columns]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Computer%20Vision%20Faces_files/figure-html/cell-7-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/johnaziz/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1930: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=3)
/Users/johnaziz/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1930: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=3)
/Users/johnaziz/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1930: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=3)
/Users/johnaziz/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1930: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=3)
/Users/johnaziz/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1930: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=3)
/Users/johnaziz/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1930: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=3)
/Users/johnaziz/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1930: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=3)
/Users/johnaziz/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1930: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=3)
/Users/johnaziz/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1930: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=3)
/Users/johnaziz/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1930: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=3)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Computer%20Vision%20Faces_files/figure-html/cell-7-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Starting t-SNE for dimensionality reduction...</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Computer%20Vision%20Faces_files/figure-html/cell-7-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Computer%20Vision%20Faces_files/figure-html/cell-7-output-9.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Computer%20Vision%20Faces_files/figure-html/cell-7-output-10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Computer%20Vision%20Faces_files/figure-html/cell-7-output-11.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Computer%20Vision%20Faces_files/figure-html/cell-7-output-12.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Computer%20Vision%20Faces_files/figure-html/cell-7-output-13.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Computer%20Vision%20Faces_files/figure-html/cell-7-output-14.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<ol type="1">
<li>Cluster Sizes:</li>
</ol>
<p>The dataset was clustered into five distinct groups using MiniBatchKMeans. Here are the sizes of each cluster:</p>
<ul>
<li><p>Cluster 3: 6578 samples</p></li>
<li><p>Cluster 1: 5723 samples</p></li>
<li><p>Cluster 4: 4735 samples</p></li>
<li><p>Cluster 2: 1791 samples</p></li>
<li><p>Cluster 0: 1433 samples</p></li>
</ul>
<ol start="2" type="1">
<li>Cluster Attribute Analysis:</li>
</ol>
<p>From the attribute analysis, we can infer general trends:</p>
<p>Cluster 0: Likely older, bald individuals with prominent facial features (e.g., big noses).</p>
<p>Cluster 1: Younger individuals with positive attributes like smiling and wearing lipstick, potentially more fashionable.</p>
<p>Cluster 2: Individuals with masculine features (sideburns, 5 o’clock shadow) but less likely to smile.</p>
<p>Cluster 3: A mix of features without strong positive or negative traits, potentially a catch-all cluster.</p>
<p>Cluster 4: Attractive individuals with more fashionable traits (smiling, lipstick, earrings).</p>
<p>The model helps in identifying distinct groupings based on facial attributes.</p>
<p>The Elbow Method graph shows how inertia (sum of squared distances within clusters) decreases as the number of clusters increases. The key observations are:</p>
<p>The inertia drops significantly between 1 and 5 clusters.</p>
<p>After 5 clusters, the rate of decrease seems to flatten out somewhat, indicating diminishing returns in reducing the sum of squared distances.</p>
<p>From this graph, 5 clusters does appear to be a good choice, as this is where the “elbow” occurs—further increases in the number of clusters do not significantly reduce the inertia.</p>
<p>The t-SNE plot gives a visual confirmation that the clustering has produced distinct groupings of attributes, though some inter-cluster mixing might still occur, particularly between Clusters 1 and 2.</p>
<p>Next, we moved on to training a generative adverserial network to try and generate new faces from the faces already in the dataset. This is an interesting approach that I explored a few years ago on the Artbreeder website.</p>
<p>The GAN consists of two main components: a Generator, which creates fake images from random noise, and a Discriminator, which attempts to differentiate between real and fake images. Over the course of training, the GAN learns to generate increasingly realistic images of faces by continuously improving both components.</p>
<div id="1976085a" class="cell" data-scrolled="true" data-tags="[]" data-execution_count="3">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets, transforms, utils <span class="im">as</span> vutils</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.cuda.amp <span class="im">import</span> GradScaler, autocast  <span class="co"># AMP for mixed precision training</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> psutil</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Set this to detect anomalies when debugging</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a><span class="co"># torch.autograd.set_detect_anomaly(False)</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Check if GPU is available</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Using device: </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a><span class="co"># CelebA Dataset path</span></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>dataset_path <span class="op">=</span> <span class="st">'/Users/johnaziz/Downloads/archive-3/img_align_celeba'</span></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Define image transformations</span></span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>    transforms.Resize((<span class="dv">32</span>, <span class="dv">32</span>)),  <span class="co"># Resize images to 32x32 (smaller size for faster training)</span></span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),  <span class="co"># Convert images to tensor</span></span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>], std<span class="op">=</span>[<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>])  <span class="co"># Normalize to [-1, 1]</span></span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Load CelebA dataset</span></span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> datasets.ImageFolder(root<span class="op">=</span>dataset_path, transform<span class="op">=</span>transform)</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>dataloader <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>)  <span class="co"># Reduced batch size for quicker iterations</span></span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Generator network (smaller version for faster training)</span></span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Generator(nn.Module):</span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Generator, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.main <span class="op">=</span> nn.Sequential(</span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a>            nn.ConvTranspose2d(<span class="dv">100</span>, <span class="dv">128</span>, <span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">0</span>, bias<span class="op">=</span><span class="va">False</span>),  <span class="co"># Reduced parameters here</span></span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(<span class="dv">128</span>),</span>
<span id="cb28-38"><a href="#cb28-38" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(<span class="va">True</span>),</span>
<span id="cb28-39"><a href="#cb28-39" aria-hidden="true" tabindex="-1"></a>            nn.ConvTranspose2d(<span class="dv">128</span>, <span class="dv">64</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb28-40"><a href="#cb28-40" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(<span class="dv">64</span>),</span>
<span id="cb28-41"><a href="#cb28-41" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(<span class="va">True</span>),</span>
<span id="cb28-42"><a href="#cb28-42" aria-hidden="true" tabindex="-1"></a>            nn.ConvTranspose2d(<span class="dv">64</span>, <span class="dv">32</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb28-43"><a href="#cb28-43" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(<span class="dv">32</span>),</span>
<span id="cb28-44"><a href="#cb28-44" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(<span class="va">True</span>),</span>
<span id="cb28-45"><a href="#cb28-45" aria-hidden="true" tabindex="-1"></a>            nn.ConvTranspose2d(<span class="dv">32</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb28-46"><a href="#cb28-46" aria-hidden="true" tabindex="-1"></a>            nn.Tanh()  <span class="co"># Output in the range [-1, 1]</span></span>
<span id="cb28-47"><a href="#cb28-47" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb28-48"><a href="#cb28-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-49"><a href="#cb28-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb28-50"><a href="#cb28-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.main(<span class="bu">input</span>)</span>
<span id="cb28-51"><a href="#cb28-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-52"><a href="#cb28-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Discriminator network (smaller version for faster training)</span></span>
<span id="cb28-53"><a href="#cb28-53" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Discriminator(nn.Module):</span>
<span id="cb28-54"><a href="#cb28-54" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb28-55"><a href="#cb28-55" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Discriminator, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb28-56"><a href="#cb28-56" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.main <span class="op">=</span> nn.Sequential(</span>
<span id="cb28-57"><a href="#cb28-57" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>),  <span class="co"># Reduced parameters</span></span>
<span id="cb28-58"><a href="#cb28-58" aria-hidden="true" tabindex="-1"></a>            nn.LeakyReLU(<span class="fl">0.2</span>, inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb28-59"><a href="#cb28-59" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb28-60"><a href="#cb28-60" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(<span class="dv">64</span>),</span>
<span id="cb28-61"><a href="#cb28-61" aria-hidden="true" tabindex="-1"></a>            nn.LeakyReLU(<span class="fl">0.2</span>, inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb28-62"><a href="#cb28-62" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb28-63"><a href="#cb28-63" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(<span class="dv">128</span>),</span>
<span id="cb28-64"><a href="#cb28-64" aria-hidden="true" tabindex="-1"></a>            nn.LeakyReLU(<span class="fl">0.2</span>, inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb28-65"><a href="#cb28-65" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">128</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">0</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb28-66"><a href="#cb28-66" aria-hidden="true" tabindex="-1"></a>            nn.Sigmoid()  <span class="co"># Output a probability score between 0 and 1</span></span>
<span id="cb28-67"><a href="#cb28-67" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb28-68"><a href="#cb28-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-69"><a href="#cb28-69" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb28-70"><a href="#cb28-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.main(<span class="bu">input</span>)</span>
<span id="cb28-71"><a href="#cb28-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-72"><a href="#cb28-72" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the networks</span></span>
<span id="cb28-73"><a href="#cb28-73" aria-hidden="true" tabindex="-1"></a>netG <span class="op">=</span> Generator().to(device)</span>
<span id="cb28-74"><a href="#cb28-74" aria-hidden="true" tabindex="-1"></a>netD <span class="op">=</span> Discriminator().to(device)</span>
<span id="cb28-75"><a href="#cb28-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-76"><a href="#cb28-76" aria-hidden="true" tabindex="-1"></a><span class="co"># Loss function</span></span>
<span id="cb28-77"><a href="#cb28-77" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.BCELoss()</span>
<span id="cb28-78"><a href="#cb28-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-79"><a href="#cb28-79" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizers</span></span>
<span id="cb28-80"><a href="#cb28-80" aria-hidden="true" tabindex="-1"></a>optimizerD <span class="op">=</span> optim.Adam(netD.parameters(), lr<span class="op">=</span><span class="fl">0.0002</span>, betas<span class="op">=</span>(<span class="fl">0.5</span>, <span class="fl">0.999</span>))</span>
<span id="cb28-81"><a href="#cb28-81" aria-hidden="true" tabindex="-1"></a>optimizerG <span class="op">=</span> optim.Adam(netG.parameters(), lr<span class="op">=</span><span class="fl">0.0002</span>, betas<span class="op">=</span>(<span class="fl">0.5</span>, <span class="fl">0.999</span>))</span>
<span id="cb28-82"><a href="#cb28-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-83"><a href="#cb28-83" aria-hidden="true" tabindex="-1"></a><span class="co"># Mixed Precision Training Scaler</span></span>
<span id="cb28-84"><a href="#cb28-84" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> GradScaler()</span>
<span id="cb28-85"><a href="#cb28-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-86"><a href="#cb28-86" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb28-87"><a href="#cb28-87" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">10</span>  <span class="co"># Reduced number of epochs</span></span>
<span id="cb28-88"><a href="#cb28-88" aria-hidden="true" tabindex="-1"></a>noise_dim <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb28-89"><a href="#cb28-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-90"><a href="#cb28-90" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Starting Training..."</span>)</span>
<span id="cb28-91"><a href="#cb28-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-92"><a href="#cb28-92" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb28-93"><a href="#cb28-93" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch [</span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_epochs<span class="sc">}</span><span class="ss">] started..."</span>)</span>
<span id="cb28-94"><a href="#cb28-94" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, data <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader, <span class="dv">0</span>):</span>
<span id="cb28-95"><a href="#cb28-95" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-96"><a href="#cb28-96" aria-hidden="true" tabindex="-1"></a>        <span class="co">############################</span></span>
<span id="cb28-97"><a href="#cb28-97" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (1) Update Discriminator</span></span>
<span id="cb28-98"><a href="#cb28-98" aria-hidden="true" tabindex="-1"></a>        <span class="co">############################</span></span>
<span id="cb28-99"><a href="#cb28-99" aria-hidden="true" tabindex="-1"></a>        netD.zero_grad()</span>
<span id="cb28-100"><a href="#cb28-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-101"><a href="#cb28-101" aria-hidden="true" tabindex="-1"></a>        real_images <span class="op">=</span> data[<span class="dv">0</span>].to(device)</span>
<span id="cb28-102"><a href="#cb28-102" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> real_images.size(<span class="dv">0</span>)</span>
<span id="cb28-103"><a href="#cb28-103" aria-hidden="true" tabindex="-1"></a>        real_labels <span class="op">=</span> torch.ones(batch_size, device<span class="op">=</span>device)</span>
<span id="cb28-104"><a href="#cb28-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-105"><a href="#cb28-105" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Mixed precision for real images</span></span>
<span id="cb28-106"><a href="#cb28-106" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> autocast():</span>
<span id="cb28-107"><a href="#cb28-107" aria-hidden="true" tabindex="-1"></a>            output_real <span class="op">=</span> netD(real_images).view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb28-108"><a href="#cb28-108" aria-hidden="true" tabindex="-1"></a>            lossD_real <span class="op">=</span> criterion(output_real, real_labels)</span>
<span id="cb28-109"><a href="#cb28-109" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-110"><a href="#cb28-110" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backprop for real images</span></span>
<span id="cb28-111"><a href="#cb28-111" aria-hidden="true" tabindex="-1"></a>        scaler.scale(lossD_real).backward()</span>
<span id="cb28-112"><a href="#cb28-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-113"><a href="#cb28-113" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate fake images</span></span>
<span id="cb28-114"><a href="#cb28-114" aria-hidden="true" tabindex="-1"></a>        noise <span class="op">=</span> torch.randn(batch_size, noise_dim, <span class="dv">1</span>, <span class="dv">1</span>, device<span class="op">=</span>device)</span>
<span id="cb28-115"><a href="#cb28-115" aria-hidden="true" tabindex="-1"></a>        fake_images <span class="op">=</span> netG(noise)</span>
<span id="cb28-116"><a href="#cb28-116" aria-hidden="true" tabindex="-1"></a>        fake_labels <span class="op">=</span> torch.zeros(batch_size, device<span class="op">=</span>device)</span>
<span id="cb28-117"><a href="#cb28-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-118"><a href="#cb28-118" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Mixed precision for fake images</span></span>
<span id="cb28-119"><a href="#cb28-119" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> autocast():</span>
<span id="cb28-120"><a href="#cb28-120" aria-hidden="true" tabindex="-1"></a>            output_fake <span class="op">=</span> netD(fake_images.detach()).view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb28-121"><a href="#cb28-121" aria-hidden="true" tabindex="-1"></a>            lossD_fake <span class="op">=</span> criterion(output_fake, fake_labels)</span>
<span id="cb28-122"><a href="#cb28-122" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-123"><a href="#cb28-123" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backprop for fake images</span></span>
<span id="cb28-124"><a href="#cb28-124" aria-hidden="true" tabindex="-1"></a>        scaler.scale(lossD_fake).backward()</span>
<span id="cb28-125"><a href="#cb28-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-126"><a href="#cb28-126" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update discriminator</span></span>
<span id="cb28-127"><a href="#cb28-127" aria-hidden="true" tabindex="-1"></a>        scaler.step(optimizerD)</span>
<span id="cb28-128"><a href="#cb28-128" aria-hidden="true" tabindex="-1"></a>        scaler.update()</span>
<span id="cb28-129"><a href="#cb28-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-130"><a href="#cb28-130" aria-hidden="true" tabindex="-1"></a>        <span class="co">############################</span></span>
<span id="cb28-131"><a href="#cb28-131" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (2) Update Generator</span></span>
<span id="cb28-132"><a href="#cb28-132" aria-hidden="true" tabindex="-1"></a>        <span class="co">############################</span></span>
<span id="cb28-133"><a href="#cb28-133" aria-hidden="true" tabindex="-1"></a>        netG.zero_grad()</span>
<span id="cb28-134"><a href="#cb28-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-135"><a href="#cb28-135" aria-hidden="true" tabindex="-1"></a>        real_labels.fill_(<span class="dv">1</span>)  <span class="co"># Generator tries to make the discriminator think the fake images are real</span></span>
<span id="cb28-136"><a href="#cb28-136" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-137"><a href="#cb28-137" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> autocast():</span>
<span id="cb28-138"><a href="#cb28-138" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> netD(fake_images).view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb28-139"><a href="#cb28-139" aria-hidden="true" tabindex="-1"></a>            lossG <span class="op">=</span> criterion(output, real_labels)</span>
<span id="cb28-140"><a href="#cb28-140" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-141"><a href="#cb28-141" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backprop for generator</span></span>
<span id="cb28-142"><a href="#cb28-142" aria-hidden="true" tabindex="-1"></a>        scaler.scale(lossG).backward()</span>
<span id="cb28-143"><a href="#cb28-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-144"><a href="#cb28-144" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update generator</span></span>
<span id="cb28-145"><a href="#cb28-145" aria-hidden="true" tabindex="-1"></a>        scaler.step(optimizerG)</span>
<span id="cb28-146"><a href="#cb28-146" aria-hidden="true" tabindex="-1"></a>        scaler.update()</span>
<span id="cb28-147"><a href="#cb28-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-148"><a href="#cb28-148" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Print loss values every 100 steps</span></span>
<span id="cb28-149"><a href="#cb28-149" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb28-150"><a href="#cb28-150" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch [</span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_epochs<span class="sc">}</span><span class="ss">], Step [</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span><span class="bu">len</span>(dataloader)<span class="sc">}</span><span class="ss">], "</span></span>
<span id="cb28-151"><a href="#cb28-151" aria-hidden="true" tabindex="-1"></a>                  <span class="ss">f"Loss D: </span><span class="sc">{</span>(lossD_real <span class="op">+</span> lossD_fake)<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">, Loss G: </span><span class="sc">{</span>lossG<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb28-152"><a href="#cb28-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-153"><a href="#cb28-153" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save generated images occasionally to monitor progress</span></span>
<span id="cb28-154"><a href="#cb28-154" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">5</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb28-155"><a href="#cb28-155" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb28-156"><a href="#cb28-156" aria-hidden="true" tabindex="-1"></a>            fake_images <span class="op">=</span> netG(noise).detach().cpu()</span>
<span id="cb28-157"><a href="#cb28-157" aria-hidden="true" tabindex="-1"></a>            vutils.save_image(fake_images, <span class="ss">f"fake_images_epoch_</span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">.png"</span>, normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-158"><a href="#cb28-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-159"><a href="#cb28-159" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the generator model at the end of training</span></span>
<span id="cb28-160"><a href="#cb28-160" aria-hidden="true" tabindex="-1"></a>model_save_path <span class="op">=</span> <span class="st">'/Users/johnaziz/Downloads/generator.pth'</span></span>
<span id="cb28-161"><a href="#cb28-161" aria-hidden="true" tabindex="-1"></a>torch.save(netG.state_dict(), model_save_path)</span>
<span id="cb28-162"><a href="#cb28-162" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model saved to </span><span class="sc">{</span>model_save_path<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Using device: cpu</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/johnaziz/anaconda3/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn(
/Users/johnaziz/anaconda3/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Starting Training...
Epoch [1/10] started...
Epoch [1/10], Step [0/6332], Loss D: 1.4791, Loss G: 0.8845
Epoch [1/10], Step [100/6332], Loss D: 0.0935, Loss G: 4.3345
Epoch [1/10], Step [200/6332], Loss D: 0.1089, Loss G: 4.4227
Epoch [1/10], Step [300/6332], Loss D: 0.5507, Loss G: 4.8454
Epoch [1/10], Step [400/6332], Loss D: 0.3992, Loss G: 3.9354
Epoch [1/10], Step [500/6332], Loss D: 0.5654, Loss G: 2.2601
Epoch [1/10], Step [600/6332], Loss D: 0.5088, Loss G: 2.4894
Epoch [1/10], Step [700/6332], Loss D: 0.4314, Loss G: 3.0759
Epoch [1/10], Step [800/6332], Loss D: 0.5845, Loss G: 1.8695
Epoch [1/10], Step [900/6332], Loss D: 0.7185, Loss G: 4.2924
Epoch [1/10], Step [1000/6332], Loss D: 0.4949, Loss G: 2.4531
Epoch [1/10], Step [1100/6332], Loss D: 0.4129, Loss G: 2.5838
Epoch [1/10], Step [1200/6332], Loss D: 0.6563, Loss G: 2.4776
Epoch [1/10], Step [1300/6332], Loss D: 0.4384, Loss G: 2.3524
Epoch [1/10], Step [1400/6332], Loss D: 0.4863, Loss G: 2.4137
Epoch [1/10], Step [1500/6332], Loss D: 0.5678, Loss G: 2.2618
Epoch [1/10], Step [1600/6332], Loss D: 0.5039, Loss G: 2.1420
Epoch [1/10], Step [1700/6332], Loss D: 0.4239, Loss G: 2.4409
Epoch [1/10], Step [1800/6332], Loss D: 0.5553, Loss G: 3.2336
Epoch [1/10], Step [1900/6332], Loss D: 0.9202, Loss G: 3.6945
Epoch [1/10], Step [2000/6332], Loss D: 0.6629, Loss G: 1.3148
Epoch [1/10], Step [2100/6332], Loss D: 0.6606, Loss G: 2.7989
Epoch [1/10], Step [2200/6332], Loss D: 0.6340, Loss G: 1.3456
Epoch [1/10], Step [2300/6332], Loss D: 0.4512, Loss G: 2.5549
Epoch [1/10], Step [2400/6332], Loss D: 0.5258, Loss G: 3.3381
Epoch [1/10], Step [2500/6332], Loss D: 0.5406, Loss G: 1.4462
Epoch [1/10], Step [2600/6332], Loss D: 0.4862, Loss G: 3.0382
Epoch [1/10], Step [2700/6332], Loss D: 0.7244, Loss G: 3.0022
Epoch [1/10], Step [2800/6332], Loss D: 0.6403, Loss G: 2.1488
Epoch [1/10], Step [2900/6332], Loss D: 0.4514, Loss G: 1.9152
Epoch [1/10], Step [3000/6332], Loss D: 0.4228, Loss G: 2.3719
Epoch [1/10], Step [3100/6332], Loss D: 0.3426, Loss G: 2.7361
Epoch [1/10], Step [3200/6332], Loss D: 0.7362, Loss G: 1.6504
Epoch [1/10], Step [3300/6332], Loss D: 0.4968, Loss G: 1.9037
Epoch [1/10], Step [3400/6332], Loss D: 0.6773, Loss G: 1.8498
Epoch [1/10], Step [3500/6332], Loss D: 0.5130, Loss G: 2.0781
Epoch [1/10], Step [3600/6332], Loss D: 1.2443, Loss G: 3.1121
Epoch [1/10], Step [3700/6332], Loss D: 0.9568, Loss G: 3.2663
Epoch [1/10], Step [3800/6332], Loss D: 0.7072, Loss G: 2.1436
Epoch [1/10], Step [3900/6332], Loss D: 0.5364, Loss G: 1.8875
Epoch [1/10], Step [4000/6332], Loss D: 0.5778, Loss G: 1.8887
Epoch [1/10], Step [4100/6332], Loss D: 0.5087, Loss G: 2.3840
Epoch [1/10], Step [4200/6332], Loss D: 0.6411, Loss G: 2.8973
Epoch [1/10], Step [4300/6332], Loss D: 0.4338, Loss G: 2.4826
Epoch [1/10], Step [4400/6332], Loss D: 0.5450, Loss G: 1.8299
Epoch [1/10], Step [4500/6332], Loss D: 0.6029, Loss G: 2.2961
Epoch [1/10], Step [4600/6332], Loss D: 0.4510, Loss G: 2.3826
Epoch [1/10], Step [4700/6332], Loss D: 0.7117, Loss G: 1.8785
Epoch [1/10], Step [4800/6332], Loss D: 0.6922, Loss G: 1.5085
Epoch [1/10], Step [4900/6332], Loss D: 0.6287, Loss G: 2.7592
Epoch [1/10], Step [5000/6332], Loss D: 0.6148, Loss G: 1.7460
Epoch [1/10], Step [5100/6332], Loss D: 0.5084, Loss G: 2.9632
Epoch [1/10], Step [5200/6332], Loss D: 0.3814, Loss G: 3.0971
Epoch [1/10], Step [5300/6332], Loss D: 0.3877, Loss G: 3.0213
Epoch [1/10], Step [5400/6332], Loss D: 1.2506, Loss G: 1.2097
Epoch [1/10], Step [5500/6332], Loss D: 0.3968, Loss G: 2.8528
Epoch [1/10], Step [5600/6332], Loss D: 1.0139, Loss G: 1.0313
Epoch [1/10], Step [5700/6332], Loss D: 0.8160, Loss G: 1.7239
Epoch [1/10], Step [5800/6332], Loss D: 0.5143, Loss G: 3.6435
Epoch [1/10], Step [5900/6332], Loss D: 0.5609, Loss G: 2.2342
Epoch [1/10], Step [6000/6332], Loss D: 0.6698, Loss G: 2.5064
Epoch [1/10], Step [6100/6332], Loss D: 0.6808, Loss G: 2.3667
Epoch [1/10], Step [6200/6332], Loss D: 0.3623, Loss G: 2.9509
Epoch [1/10], Step [6300/6332], Loss D: 0.5434, Loss G: 2.6850
Epoch [2/10] started...
Epoch [2/10], Step [0/6332], Loss D: 2.8672, Loss G: 5.2034
Epoch [2/10], Step [100/6332], Loss D: 0.4741, Loss G: 2.6944
Epoch [2/10], Step [200/6332], Loss D: 0.5191, Loss G: 1.9398
Epoch [2/10], Step [300/6332], Loss D: 0.3463, Loss G: 3.3074
Epoch [2/10], Step [400/6332], Loss D: 0.5890, Loss G: 2.0938
Epoch [2/10], Step [500/6332], Loss D: 0.4024, Loss G: 2.0501
Epoch [2/10], Step [600/6332], Loss D: 0.3597, Loss G: 2.6932
Epoch [2/10], Step [700/6332], Loss D: 0.2919, Loss G: 2.9220
Epoch [2/10], Step [800/6332], Loss D: 0.3843, Loss G: 2.3663
Epoch [2/10], Step [900/6332], Loss D: 0.4039, Loss G: 2.8179
Epoch [2/10], Step [1000/6332], Loss D: 0.5317, Loss G: 2.5969
Epoch [2/10], Step [1100/6332], Loss D: 0.3482, Loss G: 3.3842
Epoch [2/10], Step [1200/6332], Loss D: 0.6459, Loss G: 3.1657
Epoch [2/10], Step [1300/6332], Loss D: 0.6218, Loss G: 3.3386
Epoch [2/10], Step [1400/6332], Loss D: 0.4409, Loss G: 2.7347
Epoch [2/10], Step [1500/6332], Loss D: 0.2609, Loss G: 2.5856
Epoch [2/10], Step [1600/6332], Loss D: 0.5487, Loss G: 2.7877
Epoch [2/10], Step [1700/6332], Loss D: 0.6819, Loss G: 2.2784
Epoch [2/10], Step [1800/6332], Loss D: 0.4336, Loss G: 2.4793
Epoch [2/10], Step [1900/6332], Loss D: 0.4163, Loss G: 2.8477
Epoch [2/10], Step [2000/6332], Loss D: 0.4893, Loss G: 1.7003
Epoch [2/10], Step [2100/6332], Loss D: 0.9748, Loss G: 1.4941
Epoch [2/10], Step [2200/6332], Loss D: 0.4509, Loss G: 1.8979
Epoch [2/10], Step [2300/6332], Loss D: 0.3557, Loss G: 2.7009
Epoch [2/10], Step [2400/6332], Loss D: 1.0562, Loss G: 4.7098
Epoch [2/10], Step [2500/6332], Loss D: 0.4971, Loss G: 2.5099
Epoch [2/10], Step [2600/6332], Loss D: 0.5154, Loss G: 2.1980
Epoch [2/10], Step [2700/6332], Loss D: 0.4128, Loss G: 3.4445
Epoch [2/10], Step [2800/6332], Loss D: 0.3155, Loss G: 2.8088
Epoch [2/10], Step [2900/6332], Loss D: 0.6898, Loss G: 1.7747
Epoch [2/10], Step [3000/6332], Loss D: 0.2754, Loss G: 2.8969
Epoch [2/10], Step [3100/6332], Loss D: 0.3054, Loss G: 3.3895
Epoch [2/10], Step [3200/6332], Loss D: 0.5057, Loss G: 2.6706
Epoch [2/10], Step [3300/6332], Loss D: 0.3894, Loss G: 3.2817
Epoch [2/10], Step [3400/6332], Loss D: 0.5521, Loss G: 2.6485
Epoch [2/10], Step [3500/6332], Loss D: 0.3204, Loss G: 3.0912
Epoch [2/10], Step [3600/6332], Loss D: 0.6159, Loss G: 1.3736
Epoch [2/10], Step [3700/6332], Loss D: 0.3897, Loss G: 2.8536
Epoch [2/10], Step [3800/6332], Loss D: 0.4066, Loss G: 2.3769
Epoch [2/10], Step [3900/6332], Loss D: 0.3690, Loss G: 2.4061
Epoch [2/10], Step [4000/6332], Loss D: 0.2178, Loss G: 3.0279
Epoch [2/10], Step [4100/6332], Loss D: 0.2684, Loss G: 2.9352
Epoch [2/10], Step [4200/6332], Loss D: 0.2364, Loss G: 3.3390
Epoch [2/10], Step [4300/6332], Loss D: 0.7898, Loss G: 3.3599
Epoch [2/10], Step [4400/6332], Loss D: 0.2825, Loss G: 2.5547
Epoch [2/10], Step [4500/6332], Loss D: 0.3777, Loss G: 2.3690
Epoch [2/10], Step [4600/6332], Loss D: 0.4788, Loss G: 2.9052
Epoch [2/10], Step [4700/6332], Loss D: 0.7275, Loss G: 2.6770
Epoch [2/10], Step [4800/6332], Loss D: 0.3614, Loss G: 1.8719
Epoch [2/10], Step [4900/6332], Loss D: 0.4141, Loss G: 2.6488
Epoch [2/10], Step [5000/6332], Loss D: 0.2953, Loss G: 3.0621
Epoch [2/10], Step [5100/6332], Loss D: 0.4850, Loss G: 1.8247
Epoch [2/10], Step [5200/6332], Loss D: 0.3802, Loss G: 3.1834
Epoch [2/10], Step [5300/6332], Loss D: 0.3868, Loss G: 3.1072
Epoch [2/10], Step [5400/6332], Loss D: 0.4723, Loss G: 3.4868
Epoch [2/10], Step [5500/6332], Loss D: 0.4082, Loss G: 1.7541
Epoch [2/10], Step [5600/6332], Loss D: 0.3705, Loss G: 2.2633
Epoch [2/10], Step [5700/6332], Loss D: 0.6020, Loss G: 2.5319
Epoch [2/10], Step [5800/6332], Loss D: 0.5225, Loss G: 2.3053
Epoch [2/10], Step [5900/6332], Loss D: 0.5774, Loss G: 2.3316
Epoch [2/10], Step [6000/6332], Loss D: 0.3232, Loss G: 2.6180
Epoch [2/10], Step [6100/6332], Loss D: 0.2942, Loss G: 3.7645
Epoch [2/10], Step [6200/6332], Loss D: 0.3920, Loss G: 3.4719
Epoch [2/10], Step [6300/6332], Loss D: 0.3452, Loss G: 2.6147
Epoch [3/10] started...
Epoch [3/10], Step [0/6332], Loss D: 0.1865, Loss G: 3.3208
Epoch [3/10], Step [100/6332], Loss D: 0.8092, Loss G: 1.5010
Epoch [3/10], Step [200/6332], Loss D: 0.5884, Loss G: 2.0327
Epoch [3/10], Step [300/6332], Loss D: 0.5944, Loss G: 3.4794
Epoch [3/10], Step [400/6332], Loss D: 0.2201, Loss G: 3.7123
Epoch [3/10], Step [500/6332], Loss D: 0.4744, Loss G: 3.0038
Epoch [3/10], Step [600/6332], Loss D: 0.2371, Loss G: 2.1077
Epoch [3/10], Step [700/6332], Loss D: 0.8238, Loss G: 2.3663
Epoch [3/10], Step [800/6332], Loss D: 0.2254, Loss G: 2.9084
Epoch [3/10], Step [900/6332], Loss D: 0.3504, Loss G: 3.2042
Epoch [3/10], Step [1000/6332], Loss D: 0.3323, Loss G: 3.0495
Epoch [3/10], Step [1100/6332], Loss D: 0.4302, Loss G: 2.2769
Epoch [3/10], Step [1200/6332], Loss D: 0.5291, Loss G: 3.7942
Epoch [3/10], Step [1300/6332], Loss D: 0.2582, Loss G: 2.7661
Epoch [3/10], Step [1400/6332], Loss D: 0.2439, Loss G: 4.4577
Epoch [3/10], Step [1500/6332], Loss D: 0.2417, Loss G: 3.1398
Epoch [3/10], Step [1600/6332], Loss D: 0.1599, Loss G: 4.1290
Epoch [3/10], Step [1700/6332], Loss D: 0.5020, Loss G: 2.6864
Epoch [3/10], Step [1800/6332], Loss D: 0.2417, Loss G: 2.8227
Epoch [3/10], Step [1900/6332], Loss D: 0.2699, Loss G: 2.9263
Epoch [3/10], Step [2000/6332], Loss D: 0.5644, Loss G: 2.5017
Epoch [3/10], Step [2100/6332], Loss D: 0.6302, Loss G: 1.7436
Epoch [3/10], Step [2200/6332], Loss D: 0.5089, Loss G: 2.7806
Epoch [3/10], Step [2300/6332], Loss D: 0.4782, Loss G: 3.0835
Epoch [3/10], Step [2400/6332], Loss D: 0.1795, Loss G: 3.2376
Epoch [3/10], Step [2500/6332], Loss D: 0.1779, Loss G: 2.4901
Epoch [3/10], Step [2600/6332], Loss D: 0.3743, Loss G: 1.9454
Epoch [3/10], Step [2700/6332], Loss D: 0.1063, Loss G: 3.6857
Epoch [3/10], Step [2800/6332], Loss D: 0.3465, Loss G: 3.0729
Epoch [3/10], Step [2900/6332], Loss D: 0.7365, Loss G: 4.2641
Epoch [3/10], Step [3000/6332], Loss D: 0.3113, Loss G: 3.0902
Epoch [3/10], Step [3100/6332], Loss D: 0.4011, Loss G: 2.0935
Epoch [3/10], Step [3200/6332], Loss D: 0.4479, Loss G: 3.9868
Epoch [3/10], Step [3300/6332], Loss D: 0.4339, Loss G: 2.4992
Epoch [3/10], Step [3400/6332], Loss D: 1.2412, Loss G: 6.4756
Epoch [3/10], Step [3500/6332], Loss D: 0.4029, Loss G: 2.8284
Epoch [3/10], Step [3600/6332], Loss D: 0.2502, Loss G: 3.5215
Epoch [3/10], Step [3700/6332], Loss D: 0.3694, Loss G: 2.8590
Epoch [3/10], Step [3800/6332], Loss D: 0.2995, Loss G: 3.8183
Epoch [3/10], Step [3900/6332], Loss D: 0.6514, Loss G: 1.5067
Epoch [3/10], Step [4000/6332], Loss D: 0.3617, Loss G: 2.5741
Epoch [3/10], Step [4100/6332], Loss D: 0.1678, Loss G: 3.0599
Epoch [3/10], Step [4200/6332], Loss D: 0.4062, Loss G: 2.8881
Epoch [3/10], Step [4300/6332], Loss D: 0.2234, Loss G: 2.5337
Epoch [3/10], Step [4400/6332], Loss D: 0.2245, Loss G: 3.6504
Epoch [3/10], Step [4500/6332], Loss D: 0.4683, Loss G: 2.5678
Epoch [3/10], Step [4600/6332], Loss D: 0.1487, Loss G: 4.0275
Epoch [3/10], Step [4700/6332], Loss D: 0.3000, Loss G: 3.1763
Epoch [3/10], Step [4800/6332], Loss D: 0.5936, Loss G: 3.0615
Epoch [3/10], Step [4900/6332], Loss D: 0.3730, Loss G: 3.1145
Epoch [3/10], Step [5000/6332], Loss D: 0.4813, Loss G: 3.2707
Epoch [3/10], Step [5100/6332], Loss D: 0.4118, Loss G: 3.0221
Epoch [3/10], Step [5200/6332], Loss D: 0.3327, Loss G: 3.7676
Epoch [3/10], Step [5300/6332], Loss D: 0.5180, Loss G: 1.9932
Epoch [3/10], Step [5400/6332], Loss D: 0.4367, Loss G: 3.2637
Epoch [3/10], Step [5500/6332], Loss D: 0.2539, Loss G: 3.6560
Epoch [3/10], Step [5600/6332], Loss D: 0.4027, Loss G: 2.4753
Epoch [3/10], Step [5700/6332], Loss D: 0.5146, Loss G: 1.7263
Epoch [3/10], Step [5800/6332], Loss D: 0.2482, Loss G: 3.4550
Epoch [3/10], Step [5900/6332], Loss D: 0.1830, Loss G: 3.0846
Epoch [3/10], Step [6000/6332], Loss D: 0.3727, Loss G: 2.9105
Epoch [3/10], Step [6100/6332], Loss D: 0.2660, Loss G: 2.9581
Epoch [3/10], Step [6200/6332], Loss D: 0.2274, Loss G: 2.4812
Epoch [3/10], Step [6300/6332], Loss D: 0.4210, Loss G: 2.1303
Epoch [4/10] started...
Epoch [4/10], Step [0/6332], Loss D: 0.2522, Loss G: 3.9306
Epoch [4/10], Step [100/6332], Loss D: 0.2782, Loss G: 3.3759
Epoch [4/10], Step [200/6332], Loss D: 0.7407, Loss G: 1.9037
Epoch [4/10], Step [300/6332], Loss D: 0.4613, Loss G: 1.7052
Epoch [4/10], Step [400/6332], Loss D: 0.4035, Loss G: 1.5544
Epoch [4/10], Step [500/6332], Loss D: 0.9000, Loss G: 3.1637
Epoch [4/10], Step [600/6332], Loss D: 0.2899, Loss G: 2.8526
Epoch [4/10], Step [700/6332], Loss D: 0.2209, Loss G: 3.1562
Epoch [4/10], Step [800/6332], Loss D: 0.5388, Loss G: 2.7315
Epoch [4/10], Step [900/6332], Loss D: 0.3527, Loss G: 2.2839
Epoch [4/10], Step [1000/6332], Loss D: 0.4781, Loss G: 2.2349
Epoch [4/10], Step [1100/6332], Loss D: 0.4932, Loss G: 2.2005
Epoch [4/10], Step [1200/6332], Loss D: 0.2870, Loss G: 3.5245
Epoch [4/10], Step [1300/6332], Loss D: 0.3821, Loss G: 4.0307
Epoch [4/10], Step [1400/6332], Loss D: 0.2145, Loss G: 3.7395
Epoch [4/10], Step [1500/6332], Loss D: 0.2627, Loss G: 2.8152
Epoch [4/10], Step [1600/6332], Loss D: 1.9419, Loss G: 6.3310
Epoch [4/10], Step [1700/6332], Loss D: 0.1740, Loss G: 3.3208
Epoch [4/10], Step [1800/6332], Loss D: 0.3250, Loss G: 3.5745
Epoch [4/10], Step [1900/6332], Loss D: 0.1683, Loss G: 3.9170
Epoch [4/10], Step [2000/6332], Loss D: 1.5221, Loss G: 6.9799
Epoch [4/10], Step [2100/6332], Loss D: 0.5938, Loss G: 2.1057
Epoch [4/10], Step [2200/6332], Loss D: 0.2250, Loss G: 2.9947
Epoch [4/10], Step [2300/6332], Loss D: 0.4921, Loss G: 2.3787
Epoch [4/10], Step [2400/6332], Loss D: 0.1983, Loss G: 3.4762
Epoch [4/10], Step [2500/6332], Loss D: 0.2834, Loss G: 3.2913
Epoch [4/10], Step [2600/6332], Loss D: 0.3082, Loss G: 3.5972
Epoch [4/10], Step [2700/6332], Loss D: 0.3542, Loss G: 3.0345
Epoch [4/10], Step [2800/6332], Loss D: 0.4525, Loss G: 2.5429
Epoch [4/10], Step [2900/6332], Loss D: 0.1212, Loss G: 3.3179
Epoch [4/10], Step [3000/6332], Loss D: 0.2554, Loss G: 4.5496
Epoch [4/10], Step [3100/6332], Loss D: 0.1552, Loss G: 3.6792
Epoch [4/10], Step [3200/6332], Loss D: 0.3186, Loss G: 3.7478
Epoch [4/10], Step [3300/6332], Loss D: 0.3963, Loss G: 2.3342
Epoch [4/10], Step [3400/6332], Loss D: 0.6395, Loss G: 1.7755
Epoch [4/10], Step [3500/6332], Loss D: 0.1199, Loss G: 4.2309
Epoch [4/10], Step [3600/6332], Loss D: 0.5419, Loss G: 2.5167
Epoch [4/10], Step [3700/6332], Loss D: 0.6385, Loss G: 3.4959
Epoch [4/10], Step [3800/6332], Loss D: 0.1077, Loss G: 4.0110
Epoch [4/10], Step [3900/6332], Loss D: 0.3621, Loss G: 3.9402
Epoch [4/10], Step [4000/6332], Loss D: 0.4095, Loss G: 3.5967
Epoch [4/10], Step [4100/6332], Loss D: 0.5293, Loss G: 3.0294
Epoch [4/10], Step [4200/6332], Loss D: 0.2015, Loss G: 3.0370
Epoch [4/10], Step [4300/6332], Loss D: 0.3441, Loss G: 2.7469
Epoch [4/10], Step [4400/6332], Loss D: 0.3873, Loss G: 2.8199
Epoch [4/10], Step [4500/6332], Loss D: 0.2032, Loss G: 3.7017
Epoch [4/10], Step [4600/6332], Loss D: 0.2918, Loss G: 3.3486
Epoch [4/10], Step [4700/6332], Loss D: 0.1612, Loss G: 3.4548
Epoch [4/10], Step [4800/6332], Loss D: 0.4593, Loss G: 2.0868
Epoch [4/10], Step [4900/6332], Loss D: 0.3071, Loss G: 3.8359
Epoch [4/10], Step [5000/6332], Loss D: 0.3368, Loss G: 0.7124
Epoch [4/10], Step [5100/6332], Loss D: 0.2537, Loss G: 3.2606
Epoch [4/10], Step [5200/6332], Loss D: 0.3734, Loss G: 3.5746
Epoch [4/10], Step [5300/6332], Loss D: 0.3089, Loss G: 2.4754
Epoch [4/10], Step [5400/6332], Loss D: 0.1058, Loss G: 3.7114
Epoch [4/10], Step [5500/6332], Loss D: 0.4635, Loss G: 1.6887
Epoch [4/10], Step [5600/6332], Loss D: 0.3459, Loss G: 3.4917
Epoch [4/10], Step [5700/6332], Loss D: 0.5724, Loss G: 1.8701
Epoch [4/10], Step [5800/6332], Loss D: 0.0953, Loss G: 4.5743
Epoch [4/10], Step [5900/6332], Loss D: 0.1778, Loss G: 3.5126
Epoch [4/10], Step [6000/6332], Loss D: 0.2971, Loss G: 4.5752
Epoch [4/10], Step [6100/6332], Loss D: 0.3585, Loss G: 2.6138
Epoch [4/10], Step [6200/6332], Loss D: 0.2289, Loss G: 3.3800
Epoch [4/10], Step [6300/6332], Loss D: 0.4603, Loss G: 4.3569
Epoch [5/10] started...
Epoch [5/10], Step [0/6332], Loss D: 0.3060, Loss G: 3.7004
Epoch [5/10], Step [100/6332], Loss D: 0.4096, Loss G: 2.5310
Epoch [5/10], Step [200/6332], Loss D: 0.2830, Loss G: 3.1748
Epoch [5/10], Step [300/6332], Loss D: 0.3546, Loss G: 3.6217
Epoch [5/10], Step [400/6332], Loss D: 0.2128, Loss G: 3.7144
Epoch [5/10], Step [500/6332], Loss D: 0.3065, Loss G: 2.5576
Epoch [5/10], Step [600/6332], Loss D: 0.3569, Loss G: 4.3177
Epoch [5/10], Step [700/6332], Loss D: 0.3890, Loss G: 2.8254
Epoch [5/10], Step [800/6332], Loss D: 0.3010, Loss G: 3.6112
Epoch [5/10], Step [900/6332], Loss D: 0.2920, Loss G: 4.0430
Epoch [5/10], Step [1000/6332], Loss D: 1.8217, Loss G: 0.4825
Epoch [5/10], Step [1100/6332], Loss D: 0.2106, Loss G: 3.8877
Epoch [5/10], Step [1200/6332], Loss D: 0.2829, Loss G: 3.3093
Epoch [5/10], Step [1300/6332], Loss D: 0.1659, Loss G: 3.8032
Epoch [5/10], Step [1400/6332], Loss D: 0.3155, Loss G: 4.7960
Epoch [5/10], Step [1500/6332], Loss D: 0.2253, Loss G: 4.4780
Epoch [5/10], Step [1600/6332], Loss D: 0.1222, Loss G: 3.1565
Epoch [5/10], Step [1700/6332], Loss D: 0.3217, Loss G: 3.9086
Epoch [5/10], Step [1800/6332], Loss D: 0.7046, Loss G: 4.4338
Epoch [5/10], Step [1900/6332], Loss D: 0.3099, Loss G: 3.0444
Epoch [5/10], Step [2000/6332], Loss D: 0.3430, Loss G: 3.4778
Epoch [5/10], Step [2100/6332], Loss D: 0.1567, Loss G: 4.2294
Epoch [5/10], Step [2200/6332], Loss D: 0.1955, Loss G: 3.7078
Epoch [5/10], Step [2300/6332], Loss D: 0.3389, Loss G: 3.4636
Epoch [5/10], Step [2400/6332], Loss D: 0.1866, Loss G: 4.2054
Epoch [5/10], Step [2500/6332], Loss D: 0.4092, Loss G: 2.9781
Epoch [5/10], Step [2600/6332], Loss D: 0.2447, Loss G: 3.0775
Epoch [5/10], Step [2700/6332], Loss D: 0.3323, Loss G: 2.1591
Epoch [5/10], Step [2800/6332], Loss D: 0.4429, Loss G: 4.0788
Epoch [5/10], Step [2900/6332], Loss D: 0.4023, Loss G: 3.2858
Epoch [5/10], Step [3000/6332], Loss D: 0.2149, Loss G: 3.7463
Epoch [5/10], Step [3100/6332], Loss D: 0.4491, Loss G: 2.9097
Epoch [5/10], Step [3200/6332], Loss D: 0.5353, Loss G: 2.6521
Epoch [5/10], Step [3300/6332], Loss D: 0.1807, Loss G: 3.8873
Epoch [5/10], Step [3400/6332], Loss D: 0.1171, Loss G: 3.5469
Epoch [5/10], Step [3500/6332], Loss D: 0.3277, Loss G: 4.3229
Epoch [5/10], Step [3600/6332], Loss D: 0.5569, Loss G: 1.2330
Epoch [5/10], Step [3700/6332], Loss D: 0.4711, Loss G: 4.2602
Epoch [5/10], Step [3800/6332], Loss D: 0.1244, Loss G: 4.5097
Epoch [5/10], Step [3900/6332], Loss D: 0.7641, Loss G: 4.9086
Epoch [5/10], Step [4000/6332], Loss D: 0.2138, Loss G: 3.5839
Epoch [5/10], Step [4100/6332], Loss D: 0.4270, Loss G: 3.9453
Epoch [5/10], Step [4200/6332], Loss D: 0.2861, Loss G: 3.4417
Epoch [5/10], Step [4300/6332], Loss D: 0.2526, Loss G: 4.0623
Epoch [5/10], Step [4400/6332], Loss D: 0.2010, Loss G: 4.0188
Epoch [5/10], Step [4500/6332], Loss D: 0.7038, Loss G: 5.7153
Epoch [5/10], Step [4600/6332], Loss D: 0.5357, Loss G: 4.1467
Epoch [5/10], Step [4700/6332], Loss D: 0.1274, Loss G: 4.0149
Epoch [5/10], Step [4800/6332], Loss D: 0.1170, Loss G: 3.8058
Epoch [5/10], Step [4900/6332], Loss D: 0.4879, Loss G: 2.3014
Epoch [5/10], Step [5000/6332], Loss D: 0.2299, Loss G: 3.1416
Epoch [5/10], Step [5100/6332], Loss D: 0.1898, Loss G: 5.3492
Epoch [5/10], Step [5200/6332], Loss D: 0.1710, Loss G: 3.9391
Epoch [5/10], Step [5300/6332], Loss D: 0.2653, Loss G: 3.4094
Epoch [5/10], Step [5400/6332], Loss D: 0.4643, Loss G: 2.9800
Epoch [5/10], Step [5500/6332], Loss D: 0.1773, Loss G: 4.1809
Epoch [5/10], Step [5600/6332], Loss D: 0.0707, Loss G: 4.9343
Epoch [5/10], Step [5700/6332], Loss D: 0.1049, Loss G: 3.9131
Epoch [5/10], Step [5800/6332], Loss D: 1.7433, Loss G: 9.2789
Epoch [5/10], Step [5900/6332], Loss D: 0.2270, Loss G: 3.7400
Epoch [5/10], Step [6000/6332], Loss D: 0.2557, Loss G: 2.7237
Epoch [5/10], Step [6100/6332], Loss D: 0.1678, Loss G: 4.0442
Epoch [5/10], Step [6200/6332], Loss D: 0.0913, Loss G: 4.4314
Epoch [5/10], Step [6300/6332], Loss D: 0.2007, Loss G: 4.2286
Epoch [6/10] started...
Epoch [6/10], Step [0/6332], Loss D: 3.0321, Loss G: 6.3007
Epoch [6/10], Step [100/6332], Loss D: 0.2555, Loss G: 4.0060
Epoch [6/10], Step [200/6332], Loss D: 0.3187, Loss G: 3.0325
Epoch [6/10], Step [300/6332], Loss D: 0.2478, Loss G: 4.3396
Epoch [6/10], Step [400/6332], Loss D: 0.3632, Loss G: 2.2233
Epoch [6/10], Step [500/6332], Loss D: 0.1759, Loss G: 4.1635
Epoch [6/10], Step [600/6332], Loss D: 0.2031, Loss G: 3.4473
Epoch [6/10], Step [700/6332], Loss D: 0.3363, Loss G: 2.3271
Epoch [6/10], Step [800/6332], Loss D: 0.5469, Loss G: 3.7607
Epoch [6/10], Step [900/6332], Loss D: 0.2569, Loss G: 2.9773
Epoch [6/10], Step [1000/6332], Loss D: 0.1676, Loss G: 3.1484
Epoch [6/10], Step [1100/6332], Loss D: 0.4840, Loss G: 2.7115
Epoch [6/10], Step [1200/6332], Loss D: 0.2113, Loss G: 3.7091
Epoch [6/10], Step [1300/6332], Loss D: 0.2060, Loss G: 3.4633
Epoch [6/10], Step [1400/6332], Loss D: 0.1039, Loss G: 3.9909
Epoch [6/10], Step [1500/6332], Loss D: 0.1573, Loss G: 4.2367
Epoch [6/10], Step [1600/6332], Loss D: 0.0909, Loss G: 4.3255
Epoch [6/10], Step [1700/6332], Loss D: 0.4053, Loss G: 3.4344
Epoch [6/10], Step [1800/6332], Loss D: 0.1863, Loss G: 4.1920
Epoch [6/10], Step [1900/6332], Loss D: 0.4132, Loss G: 4.1119
Epoch [6/10], Step [2000/6332], Loss D: 0.2478, Loss G: 3.3482
Epoch [6/10], Step [2100/6332], Loss D: 0.1482, Loss G: 3.1382
Epoch [6/10], Step [2200/6332], Loss D: 0.6367, Loss G: 4.0037
Epoch [6/10], Step [2300/6332], Loss D: 0.0361, Loss G: 5.4335
Epoch [6/10], Step [2400/6332], Loss D: 0.2019, Loss G: 4.0369
Epoch [6/10], Step [2500/6332], Loss D: 0.2204, Loss G: 2.9912
Epoch [6/10], Step [2600/6332], Loss D: 0.6524, Loss G: 5.0678
Epoch [6/10], Step [2700/6332], Loss D: 0.3852, Loss G: 2.7263
Epoch [6/10], Step [2800/6332], Loss D: 0.5733, Loss G: 3.5383
Epoch [6/10], Step [2900/6332], Loss D: 0.2902, Loss G: 3.6298
Epoch [6/10], Step [3000/6332], Loss D: 0.2400, Loss G: 4.3139
Epoch [6/10], Step [3100/6332], Loss D: 0.3908, Loss G: 4.4035
Epoch [6/10], Step [3200/6332], Loss D: 0.3955, Loss G: 3.7704
Epoch [6/10], Step [3300/6332], Loss D: 0.2511, Loss G: 3.0227
Epoch [6/10], Step [3400/6332], Loss D: 0.8969, Loss G: 0.9989
Epoch [6/10], Step [3500/6332], Loss D: 0.0493, Loss G: 4.6888
Epoch [6/10], Step [3600/6332], Loss D: 0.1747, Loss G: 3.3942
Epoch [6/10], Step [3700/6332], Loss D: 0.0658, Loss G: 3.9136
Epoch [6/10], Step [3800/6332], Loss D: 0.3011, Loss G: 3.0780
Epoch [6/10], Step [3900/6332], Loss D: 0.2250, Loss G: 3.4658
Epoch [6/10], Step [4000/6332], Loss D: 0.4848, Loss G: 5.6663
Epoch [6/10], Step [4100/6332], Loss D: 0.1348, Loss G: 3.9236
Epoch [6/10], Step [4200/6332], Loss D: 0.3121, Loss G: 4.1593
Epoch [6/10], Step [4300/6332], Loss D: 1.3603, Loss G: 5.8553
Epoch [6/10], Step [4400/6332], Loss D: 0.3603, Loss G: 3.7636
Epoch [6/10], Step [4500/6332], Loss D: 0.1984, Loss G: 3.3912
Epoch [6/10], Step [4600/6332], Loss D: 0.1755, Loss G: 4.3470
Epoch [6/10], Step [4700/6332], Loss D: 0.2055, Loss G: 3.7602
Epoch [6/10], Step [4800/6332], Loss D: 0.2414, Loss G: 4.0171
Epoch [6/10], Step [4900/6332], Loss D: 0.4485, Loss G: 3.7171
Epoch [6/10], Step [5000/6332], Loss D: 0.1210, Loss G: 4.0152
Epoch [6/10], Step [5100/6332], Loss D: 0.2510, Loss G: 4.1112
Epoch [6/10], Step [5200/6332], Loss D: 0.2020, Loss G: 3.5891
Epoch [6/10], Step [5300/6332], Loss D: 0.3275, Loss G: 4.9528
Epoch [6/10], Step [5400/6332], Loss D: 0.5270, Loss G: 3.0126
Epoch [6/10], Step [5500/6332], Loss D: 0.6409, Loss G: 2.1241
Epoch [6/10], Step [5600/6332], Loss D: 0.3778, Loss G: 3.8509
Epoch [6/10], Step [5700/6332], Loss D: 0.1640, Loss G: 3.4485
Epoch [6/10], Step [5800/6332], Loss D: 0.2362, Loss G: 3.1177
Epoch [6/10], Step [5900/6332], Loss D: 0.4642, Loss G: 2.3580
Epoch [6/10], Step [6000/6332], Loss D: 0.2771, Loss G: 5.0135
Epoch [6/10], Step [6100/6332], Loss D: 0.6392, Loss G: 1.4088
Epoch [6/10], Step [6200/6332], Loss D: 0.2764, Loss G: 4.2049
Epoch [6/10], Step [6300/6332], Loss D: 0.1393, Loss G: 4.3831
Epoch [7/10] started...
Epoch [7/10], Step [0/6332], Loss D: 3.4621, Loss G: 7.1320
Epoch [7/10], Step [100/6332], Loss D: 0.3469, Loss G: 3.7959
Epoch [7/10], Step [200/6332], Loss D: 0.2194, Loss G: 3.4739
Epoch [7/10], Step [300/6332], Loss D: 0.3248, Loss G: 2.7253
Epoch [7/10], Step [400/6332], Loss D: 0.4533, Loss G: 6.4653
Epoch [7/10], Step [500/6332], Loss D: 0.3414, Loss G: 3.6742
Epoch [7/10], Step [600/6332], Loss D: 0.2309, Loss G: 2.7881
Epoch [7/10], Step [700/6332], Loss D: 0.2945, Loss G: 2.4251
Epoch [7/10], Step [800/6332], Loss D: 0.1938, Loss G: 3.3689
Epoch [7/10], Step [900/6332], Loss D: 0.1680, Loss G: 3.7697
Epoch [7/10], Step [1000/6332], Loss D: 0.1664, Loss G: 3.8989
Epoch [7/10], Step [1100/6332], Loss D: 0.2797, Loss G: 3.8675
Epoch [7/10], Step [1200/6332], Loss D: 0.2583, Loss G: 3.0975
Epoch [7/10], Step [1300/6332], Loss D: 0.2408, Loss G: 4.2330
Epoch [7/10], Step [1400/6332], Loss D: 0.6282, Loss G: 3.9344
Epoch [7/10], Step [1500/6332], Loss D: 0.3942, Loss G: 3.7606
Epoch [7/10], Step [1600/6332], Loss D: 0.1271, Loss G: 3.5091
Epoch [7/10], Step [1700/6332], Loss D: 1.0854, Loss G: 7.0034
Epoch [7/10], Step [1800/6332], Loss D: 0.0860, Loss G: 4.0160
Epoch [7/10], Step [1900/6332], Loss D: 0.2071, Loss G: 3.0413
Epoch [7/10], Step [2000/6332], Loss D: 0.2421, Loss G: 4.2091
Epoch [7/10], Step [2100/6332], Loss D: 0.4399, Loss G: 2.6384
Epoch [7/10], Step [2200/6332], Loss D: 0.2130, Loss G: 3.5946
Epoch [7/10], Step [2300/6332], Loss D: 0.2112, Loss G: 3.2351
Epoch [7/10], Step [2400/6332], Loss D: 0.1397, Loss G: 4.3080
Epoch [7/10], Step [2500/6332], Loss D: 0.4609, Loss G: 4.8825
Epoch [7/10], Step [2600/6332], Loss D: 0.1176, Loss G: 4.8167
Epoch [7/10], Step [2700/6332], Loss D: 0.2380, Loss G: 4.7794
Epoch [7/10], Step [2800/6332], Loss D: 0.1203, Loss G: 4.5869
Epoch [7/10], Step [2900/6332], Loss D: 0.1495, Loss G: 4.1899
Epoch [7/10], Step [3000/6332], Loss D: 0.5335, Loss G: 2.2162
Epoch [7/10], Step [3100/6332], Loss D: 0.2619, Loss G: 4.8232
Epoch [7/10], Step [3200/6332], Loss D: 0.0763, Loss G: 4.5319
Epoch [7/10], Step [3300/6332], Loss D: 0.0929, Loss G: 4.1052
Epoch [7/10], Step [3400/6332], Loss D: 0.2642, Loss G: 3.4676
Epoch [7/10], Step [3500/6332], Loss D: 0.3958, Loss G: 2.9484
Epoch [7/10], Step [3600/6332], Loss D: 0.6361, Loss G: 4.6880
Epoch [7/10], Step [3700/6332], Loss D: 0.2019, Loss G: 4.4548
Epoch [7/10], Step [3800/6332], Loss D: 0.0790, Loss G: 4.1901
Epoch [7/10], Step [3900/6332], Loss D: 0.3962, Loss G: 3.0262
Epoch [7/10], Step [4000/6332], Loss D: 0.4934, Loss G: 4.0621
Epoch [7/10], Step [4100/6332], Loss D: 0.1243, Loss G: 4.5555
Epoch [7/10], Step [4200/6332], Loss D: 0.1312, Loss G: 4.4515
Epoch [7/10], Step [4300/6332], Loss D: 0.2105, Loss G: 3.6124
Epoch [7/10], Step [4400/6332], Loss D: 0.2505, Loss G: 3.6218
Epoch [7/10], Step [4500/6332], Loss D: 0.1128, Loss G: 4.2012
Epoch [7/10], Step [4600/6332], Loss D: 0.2572, Loss G: 3.6019
Epoch [7/10], Step [4700/6332], Loss D: 0.2382, Loss G: 3.5985
Epoch [7/10], Step [4800/6332], Loss D: 0.2979, Loss G: 3.0777
Epoch [7/10], Step [4900/6332], Loss D: 0.1053, Loss G: 5.0069
Epoch [7/10], Step [5000/6332], Loss D: 0.1321, Loss G: 4.4976
Epoch [7/10], Step [5100/6332], Loss D: 0.1052, Loss G: 4.9631
Epoch [7/10], Step [5200/6332], Loss D: 0.2189, Loss G: 4.4387
Epoch [7/10], Step [5300/6332], Loss D: 0.1089, Loss G: 3.8893
Epoch [7/10], Step [5400/6332], Loss D: 0.1824, Loss G: 3.3430
Epoch [7/10], Step [5500/6332], Loss D: 0.2172, Loss G: 3.7388
Epoch [7/10], Step [5600/6332], Loss D: 0.3427, Loss G: 4.6270
Epoch [7/10], Step [5700/6332], Loss D: 0.1890, Loss G: 3.4416
Epoch [7/10], Step [5800/6332], Loss D: 0.1878, Loss G: 3.9971
Epoch [7/10], Step [5900/6332], Loss D: 0.0840, Loss G: 4.3802
Epoch [7/10], Step [6000/6332], Loss D: 0.2976, Loss G: 2.4572
Epoch [7/10], Step [6100/6332], Loss D: 0.1535, Loss G: 3.4398
Epoch [7/10], Step [6200/6332], Loss D: 0.1121, Loss G: 4.0775
Epoch [7/10], Step [6300/6332], Loss D: 0.1830, Loss G: 3.8593
Epoch [8/10] started...
Epoch [8/10], Step [0/6332], Loss D: 0.2130, Loss G: 3.7172
Epoch [8/10], Step [100/6332], Loss D: 0.0577, Loss G: 4.3148
Epoch [8/10], Step [200/6332], Loss D: 0.2790, Loss G: 4.0680
Epoch [8/10], Step [300/6332], Loss D: 0.1451, Loss G: 4.8905
Epoch [8/10], Step [400/6332], Loss D: 0.1132, Loss G: 4.3270
Epoch [8/10], Step [500/6332], Loss D: 0.2647, Loss G: 4.4467
Epoch [8/10], Step [600/6332], Loss D: 0.1738, Loss G: 5.0740
Epoch [8/10], Step [700/6332], Loss D: 0.1552, Loss G: 3.9201
Epoch [8/10], Step [800/6332], Loss D: 0.9731, Loss G: 5.6172
Epoch [8/10], Step [900/6332], Loss D: 0.1947, Loss G: 4.1400
Epoch [8/10], Step [1000/6332], Loss D: 0.1173, Loss G: 4.6967
Epoch [8/10], Step [1100/6332], Loss D: 0.2496, Loss G: 3.1907
Epoch [8/10], Step [1200/6332], Loss D: 0.2263, Loss G: 3.7010
Epoch [8/10], Step [1300/6332], Loss D: 0.1463, Loss G: 4.7577
Epoch [8/10], Step [1400/6332], Loss D: 0.3028, Loss G: 4.1780
Epoch [8/10], Step [1500/6332], Loss D: 0.1297, Loss G: 4.9076
Epoch [8/10], Step [1600/6332], Loss D: 0.5398, Loss G: 3.9623
Epoch [8/10], Step [1700/6332], Loss D: 0.0790, Loss G: 5.2504
Epoch [8/10], Step [1800/6332], Loss D: 0.1692, Loss G: 4.3848
Epoch [8/10], Step [1900/6332], Loss D: 0.1283, Loss G: 4.7922
Epoch [8/10], Step [2000/6332], Loss D: 0.7677, Loss G: 6.9229
Epoch [8/10], Step [2100/6332], Loss D: 0.0871, Loss G: 4.2675
Epoch [8/10], Step [2200/6332], Loss D: 0.1690, Loss G: 3.6757
Epoch [8/10], Step [2300/6332], Loss D: 0.4790, Loss G: 2.9094
Epoch [8/10], Step [2400/6332], Loss D: 0.0907, Loss G: 4.9413
Epoch [8/10], Step [2500/6332], Loss D: 0.1099, Loss G: 3.8527
Epoch [8/10], Step [2600/6332], Loss D: 0.5140, Loss G: 6.2726
Epoch [8/10], Step [2700/6332], Loss D: 0.2069, Loss G: 4.3824
Epoch [8/10], Step [2800/6332], Loss D: 0.3640, Loss G: 3.3173
Epoch [8/10], Step [2900/6332], Loss D: 0.1127, Loss G: 4.0219
Epoch [8/10], Step [3000/6332], Loss D: 0.1856, Loss G: 3.7562
Epoch [8/10], Step [3100/6332], Loss D: 0.1447, Loss G: 4.1205
Epoch [8/10], Step [3200/6332], Loss D: 0.3353, Loss G: 3.6733
Epoch [8/10], Step [3300/6332], Loss D: 0.1687, Loss G: 4.2870
Epoch [8/10], Step [3400/6332], Loss D: 0.1595, Loss G: 4.1839
Epoch [8/10], Step [3500/6332], Loss D: 0.1329, Loss G: 3.6725
Epoch [8/10], Step [3600/6332], Loss D: 0.1436, Loss G: 3.4988
Epoch [8/10], Step [3700/6332], Loss D: 0.1787, Loss G: 4.3541
Epoch [8/10], Step [3800/6332], Loss D: 0.0732, Loss G: 3.8100
Epoch [8/10], Step [3900/6332], Loss D: 0.3103, Loss G: 3.3375
Epoch [8/10], Step [4000/6332], Loss D: 0.0830, Loss G: 4.8548
Epoch [8/10], Step [4100/6332], Loss D: 0.2438, Loss G: 4.2697
Epoch [8/10], Step [4200/6332], Loss D: 0.1662, Loss G: 3.6356
Epoch [8/10], Step [4300/6332], Loss D: 0.2641, Loss G: 4.8947
Epoch [8/10], Step [4400/6332], Loss D: 0.2108, Loss G: 3.8395
Epoch [8/10], Step [4500/6332], Loss D: 0.4754, Loss G: 6.6611
Epoch [8/10], Step [4600/6332], Loss D: 0.3035, Loss G: 3.3801
Epoch [8/10], Step [4700/6332], Loss D: 0.1601, Loss G: 4.6466
Epoch [8/10], Step [4800/6332], Loss D: 0.0985, Loss G: 3.1184
Epoch [8/10], Step [4900/6332], Loss D: 0.2484, Loss G: 3.2003
Epoch [8/10], Step [5000/6332], Loss D: 0.1068, Loss G: 4.5772
Epoch [8/10], Step [5100/6332], Loss D: 0.6598, Loss G: 1.8816
Epoch [8/10], Step [5200/6332], Loss D: 0.1188, Loss G: 4.5127
Epoch [8/10], Step [5300/6332], Loss D: 0.5482, Loss G: 4.4398
Epoch [8/10], Step [5400/6332], Loss D: 0.3877, Loss G: 3.3901
Epoch [8/10], Step [5500/6332], Loss D: 0.0859, Loss G: 5.2975
Epoch [8/10], Step [5600/6332], Loss D: 0.1391, Loss G: 4.5608
Epoch [8/10], Step [5700/6332], Loss D: 0.7104, Loss G: 7.4148
Epoch [8/10], Step [5800/6332], Loss D: 0.1883, Loss G: 3.9325
Epoch [8/10], Step [5900/6332], Loss D: 0.4206, Loss G: 5.3310
Epoch [8/10], Step [6000/6332], Loss D: 0.2313, Loss G: 4.4372
Epoch [8/10], Step [6100/6332], Loss D: 0.1888, Loss G: 3.2788
Epoch [8/10], Step [6200/6332], Loss D: 0.1430, Loss G: 3.8189
Epoch [8/10], Step [6300/6332], Loss D: 0.1008, Loss G: 4.4676
Epoch [9/10] started...
Epoch [9/10], Step [0/6332], Loss D: 0.0996, Loss G: 5.4527
Epoch [9/10], Step [100/6332], Loss D: 0.1026, Loss G: 4.5926
Epoch [9/10], Step [200/6332], Loss D: 0.5848, Loss G: 5.8617
Epoch [9/10], Step [300/6332], Loss D: 0.1733, Loss G: 4.3315
Epoch [9/10], Step [400/6332], Loss D: 0.1687, Loss G: 4.0742
Epoch [9/10], Step [500/6332], Loss D: 0.0499, Loss G: 4.9899
Epoch [9/10], Step [600/6332], Loss D: 0.1096, Loss G: 4.0689
Epoch [9/10], Step [700/6332], Loss D: 0.1682, Loss G: 4.8917
Epoch [9/10], Step [800/6332], Loss D: 0.1024, Loss G: 5.3511
Epoch [9/10], Step [900/6332], Loss D: 0.0849, Loss G: 4.1159
Epoch [9/10], Step [1000/6332], Loss D: 0.1150, Loss G: 3.5242
Epoch [9/10], Step [1100/6332], Loss D: 0.1082, Loss G: 4.6995
Epoch [9/10], Step [1200/6332], Loss D: 0.2379, Loss G: 3.5078
Epoch [9/10], Step [1300/6332], Loss D: 0.3062, Loss G: 5.4871
Epoch [9/10], Step [1400/6332], Loss D: 0.0926, Loss G: 3.5419
Epoch [9/10], Step [1500/6332], Loss D: 0.2032, Loss G: 3.5446
Epoch [9/10], Step [1600/6332], Loss D: 0.2928, Loss G: 4.1329
Epoch [9/10], Step [1700/6332], Loss D: 0.1143, Loss G: 3.7568
Epoch [9/10], Step [1800/6332], Loss D: 0.1341, Loss G: 4.3896
Epoch [9/10], Step [1900/6332], Loss D: 0.0586, Loss G: 5.1071
Epoch [9/10], Step [2000/6332], Loss D: 0.1923, Loss G: 4.2066
Epoch [9/10], Step [2100/6332], Loss D: 0.1841, Loss G: 4.1111
Epoch [9/10], Step [2200/6332], Loss D: 0.0804, Loss G: 3.8457
Epoch [9/10], Step [2300/6332], Loss D: 0.2630, Loss G: 2.1351
Epoch [9/10], Step [2400/6332], Loss D: 0.1345, Loss G: 3.7691
Epoch [9/10], Step [2500/6332], Loss D: 0.2757, Loss G: 3.4348
Epoch [9/10], Step [2600/6332], Loss D: 0.2044, Loss G: 4.4574
Epoch [9/10], Step [2700/6332], Loss D: 0.3714, Loss G: 3.5034
Epoch [9/10], Step [2800/6332], Loss D: 0.2043, Loss G: 3.3945
Epoch [9/10], Step [2900/6332], Loss D: 0.1035, Loss G: 4.0647
Epoch [9/10], Step [3000/6332], Loss D: 0.2540, Loss G: 4.0938
Epoch [9/10], Step [3100/6332], Loss D: 0.2836, Loss G: 5.5653
Epoch [9/10], Step [3200/6332], Loss D: 0.1376, Loss G: 4.0812
Epoch [9/10], Step [3300/6332], Loss D: 0.2270, Loss G: 2.8617
Epoch [9/10], Step [3400/6332], Loss D: 0.1153, Loss G: 4.0286
Epoch [9/10], Step [3500/6332], Loss D: 0.0774, Loss G: 5.4191
Epoch [9/10], Step [3600/6332], Loss D: 0.1415, Loss G: 3.4432
Epoch [9/10], Step [3700/6332], Loss D: 0.3243, Loss G: 2.5641
Epoch [9/10], Step [3800/6332], Loss D: 0.1170, Loss G: 5.2387
Epoch [9/10], Step [3900/6332], Loss D: 0.3146, Loss G: 5.1497
Epoch [9/10], Step [4000/6332], Loss D: 0.1065, Loss G: 3.5945
Epoch [9/10], Step [4100/6332], Loss D: 0.1157, Loss G: 4.9469
Epoch [9/10], Step [4200/6332], Loss D: 0.3985, Loss G: 1.8981
Epoch [9/10], Step [4300/6332], Loss D: 0.1375, Loss G: 3.1035
Epoch [9/10], Step [4400/6332], Loss D: 0.1181, Loss G: 4.6269
Epoch [9/10], Step [4500/6332], Loss D: 0.1099, Loss G: 4.4660
Epoch [9/10], Step [4600/6332], Loss D: 0.0882, Loss G: 4.4135
Epoch [9/10], Step [4700/6332], Loss D: 0.3438, Loss G: 4.3483
Epoch [9/10], Step [4800/6332], Loss D: 0.2070, Loss G: 3.8017
Epoch [9/10], Step [4900/6332], Loss D: 0.1611, Loss G: 3.7107
Epoch [9/10], Step [5000/6332], Loss D: 0.1723, Loss G: 3.1637
Epoch [9/10], Step [5100/6332], Loss D: 0.4103, Loss G: 4.7495
Epoch [9/10], Step [5200/6332], Loss D: 0.2959, Loss G: 3.2813
Epoch [9/10], Step [5300/6332], Loss D: 0.1095, Loss G: 3.7747
Epoch [9/10], Step [5400/6332], Loss D: 0.4264, Loss G: 5.8806
Epoch [9/10], Step [5500/6332], Loss D: 0.1673, Loss G: 5.6795
Epoch [9/10], Step [5600/6332], Loss D: 0.1468, Loss G: 5.2509
Epoch [9/10], Step [5700/6332], Loss D: 0.2448, Loss G: 3.6232
Epoch [9/10], Step [5800/6332], Loss D: 0.2207, Loss G: 4.2975
Epoch [9/10], Step [5900/6332], Loss D: 0.2534, Loss G: 3.7856
Epoch [9/10], Step [6000/6332], Loss D: 0.0623, Loss G: 4.7303
Epoch [9/10], Step [6100/6332], Loss D: 0.2337, Loss G: 3.4441
Epoch [9/10], Step [6200/6332], Loss D: 0.0795, Loss G: 4.3176
Epoch [9/10], Step [6300/6332], Loss D: 0.1722, Loss G: 3.8105
Epoch [10/10] started...
Epoch [10/10], Step [0/6332], Loss D: 0.0884, Loss G: 4.9404
Epoch [10/10], Step [100/6332], Loss D: 0.1304, Loss G: 5.5374
Epoch [10/10], Step [200/6332], Loss D: 0.0922, Loss G: 4.8592
Epoch [10/10], Step [300/6332], Loss D: 0.0979, Loss G: 4.1861
Epoch [10/10], Step [400/6332], Loss D: 0.0880, Loss G: 4.6996
Epoch [10/10], Step [500/6332], Loss D: 0.0993, Loss G: 5.0975
Epoch [10/10], Step [600/6332], Loss D: 0.3388, Loss G: 3.5679
Epoch [10/10], Step [700/6332], Loss D: 0.2176, Loss G: 4.1718
Epoch [10/10], Step [800/6332], Loss D: 0.1972, Loss G: 3.7692
Epoch [10/10], Step [900/6332], Loss D: 0.4420, Loss G: 3.8423
Epoch [10/10], Step [1000/6332], Loss D: 0.0977, Loss G: 5.0571
Epoch [10/10], Step [1100/6332], Loss D: 0.1270, Loss G: 3.9346
Epoch [10/10], Step [1200/6332], Loss D: 0.2761, Loss G: 3.1488
Epoch [10/10], Step [1300/6332], Loss D: 0.3766, Loss G: 5.7111
Epoch [10/10], Step [1400/6332], Loss D: 0.0936, Loss G: 4.3459
Epoch [10/10], Step [1500/6332], Loss D: 0.1446, Loss G: 4.3162
Epoch [10/10], Step [1600/6332], Loss D: 0.0542, Loss G: 4.5707
Epoch [10/10], Step [1700/6332], Loss D: 0.0911, Loss G: 4.1303
Epoch [10/10], Step [1800/6332], Loss D: 0.1867, Loss G: 4.2266
Epoch [10/10], Step [1900/6332], Loss D: 0.1044, Loss G: 5.5910
Epoch [10/10], Step [2000/6332], Loss D: 0.3644, Loss G: 4.8536
Epoch [10/10], Step [2100/6332], Loss D: 0.3029, Loss G: 2.1281
Epoch [10/10], Step [2200/6332], Loss D: 0.0751, Loss G: 4.7627
Epoch [10/10], Step [2300/6332], Loss D: 0.1168, Loss G: 4.2392
Epoch [10/10], Step [2400/6332], Loss D: 0.0641, Loss G: 4.5900
Epoch [10/10], Step [2500/6332], Loss D: 0.1145, Loss G: 5.7441
Epoch [10/10], Step [2600/6332], Loss D: 0.4095, Loss G: 4.2876
Epoch [10/10], Step [2700/6332], Loss D: 0.2700, Loss G: 3.5258
Epoch [10/10], Step [2800/6332], Loss D: 0.1507, Loss G: 4.0041
Epoch [10/10], Step [2900/6332], Loss D: 0.2508, Loss G: 5.0734
Epoch [10/10], Step [3000/6332], Loss D: 0.3087, Loss G: 2.1465
Epoch [10/10], Step [3100/6332], Loss D: 0.1266, Loss G: 3.7860
Epoch [10/10], Step [3200/6332], Loss D: 0.1155, Loss G: 4.6051
Epoch [10/10], Step [3300/6332], Loss D: 0.0893, Loss G: 3.9620
Epoch [10/10], Step [3400/6332], Loss D: 0.0648, Loss G: 3.9662
Epoch [10/10], Step [3500/6332], Loss D: 0.2862, Loss G: 4.7489
Epoch [10/10], Step [3600/6332], Loss D: 0.1891, Loss G: 4.3997
Epoch [10/10], Step [3700/6332], Loss D: 0.2363, Loss G: 2.8241
Epoch [10/10], Step [3800/6332], Loss D: 0.2065, Loss G: 4.2976
Epoch [10/10], Step [3900/6332], Loss D: 0.1084, Loss G: 4.5697
Epoch [10/10], Step [4000/6332], Loss D: 0.1553, Loss G: 5.1091
Epoch [10/10], Step [4100/6332], Loss D: 0.2161, Loss G: 4.2622
Epoch [10/10], Step [4200/6332], Loss D: 0.0638, Loss G: 4.9843
Epoch [10/10], Step [4300/6332], Loss D: 0.0785, Loss G: 4.6381
Epoch [10/10], Step [4400/6332], Loss D: 0.2147, Loss G: 3.5305
Epoch [10/10], Step [4500/6332], Loss D: 0.1843, Loss G: 4.1189
Epoch [10/10], Step [4600/6332], Loss D: 0.2296, Loss G: 4.1477
Epoch [10/10], Step [4700/6332], Loss D: 0.3089, Loss G: 3.7942
Epoch [10/10], Step [4800/6332], Loss D: 0.1483, Loss G: 4.8902
Epoch [10/10], Step [4900/6332], Loss D: 0.0439, Loss G: 4.6812
Epoch [10/10], Step [5000/6332], Loss D: 0.2879, Loss G: 4.8884
Epoch [10/10], Step [5100/6332], Loss D: 0.2474, Loss G: 4.6356
Epoch [10/10], Step [5200/6332], Loss D: 0.0744, Loss G: 5.0035
Epoch [10/10], Step [5300/6332], Loss D: 0.2357, Loss G: 3.6538
Epoch [10/10], Step [5400/6332], Loss D: 0.0893, Loss G: 3.3938
Epoch [10/10], Step [5500/6332], Loss D: 0.1958, Loss G: 3.3703
Epoch [10/10], Step [5600/6332], Loss D: 0.1685, Loss G: 3.9142
Epoch [10/10], Step [5700/6332], Loss D: 0.0514, Loss G: 4.9965
Epoch [10/10], Step [5800/6332], Loss D: 0.1423, Loss G: 5.0588
Epoch [10/10], Step [5900/6332], Loss D: 0.0810, Loss G: 4.5951
Epoch [10/10], Step [6000/6332], Loss D: 0.1909, Loss G: 3.9457
Epoch [10/10], Step [6100/6332], Loss D: 0.0792, Loss G: 5.7829
Epoch [10/10], Step [6200/6332], Loss D: 0.3012, Loss G: 4.4089
Epoch [10/10], Step [6300/6332], Loss D: 0.1659, Loss G: 3.9040
Model saved to /Users/johnaziz/Downloads/generator.pth</code></pre>
</div>
</div>
<p>Over the 10 epochs, both the Generator and Discriminator improved significantly. Early in training, the Discriminator could easily distinguish fake from real images (indicated by high Generator loss), but as training progressed, the Generator’s images became more realistic, and the Discriminator found it more challenging to differentiate between the two (indicated by lower Discriminator loss).</p>
<p>Loss oscillations in later epochs indicated the model still had room for improvement, particularly in stabilizing the training. GANs are known for being challenging to train due to their adversarial nature, and the balance between the Generator and Discriminator is crucial for achieving optimal results.</p>
<p>Next, we tested out the generation, including generating some faces:</p>
<div id="c02b27d6" class="cell" data-tags="[]" data-execution_count="8">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.utils <span class="im">import</span> make_grid</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to denormalize the images back to the [0, 1] range</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> denormalize(img_tensor):</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>    img_tensor <span class="op">=</span> img_tensor <span class="op">*</span> <span class="fl">0.5</span> <span class="op">+</span> <span class="fl">0.5</span>  <span class="co"># Reverse normalization (mean=0.5, std=0.5)</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> img_tensor</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to upscale images</span></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> upscale_images(images, upscale_factor<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Rescale images using bicubic interpolation</span></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>    upscaled_images <span class="op">=</span> F.interpolate(images, scale_factor<span class="op">=</span>upscale_factor, mode<span class="op">=</span><span class="st">'bicubic'</span>, align_corners<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> upscaled_images</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to generate and visualize fake faces with upscaling</span></span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_faces(generator, num_images<span class="op">=</span><span class="dv">16</span>, noise_dim<span class="op">=</span><span class="dv">100</span>, device<span class="op">=</span><span class="st">"cpu"</span>, upscale_factor<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set generator to evaluation mode</span></span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>    generator.<span class="bu">eval</span>()</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate random noise</span></span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>    noise <span class="op">=</span> torch.randn(num_images, noise_dim, <span class="dv">1</span>, <span class="dv">1</span>, device<span class="op">=</span>device)</span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate fake images from noise</span></span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a>        fake_images <span class="op">=</span> generator(noise).detach().cpu()</span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Denormalize images to [0, 1] range</span></span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a>    fake_images <span class="op">=</span> denormalize(fake_images)</span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Upscale the images to a higher resolution</span></span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a>    upscaled_images <span class="op">=</span> upscale_images(fake_images, upscale_factor<span class="op">=</span>upscale_factor)</span>
<span id="cb32-35"><a href="#cb32-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-36"><a href="#cb32-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the upscaled generated images in a grid</span></span>
<span id="cb32-37"><a href="#cb32-37" aria-hidden="true" tabindex="-1"></a>    grid <span class="op">=</span> make_grid(upscaled_images, nrow<span class="op">=</span><span class="bu">int</span>(np.sqrt(num_images)), normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-38"><a href="#cb32-38" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))  <span class="co"># Larger figure for better resolution</span></span>
<span id="cb32-39"><a href="#cb32-39" aria-hidden="true" tabindex="-1"></a>    plt.imshow(np.transpose(grid.numpy(), (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))  <span class="co"># Convert (C, H, W) to (H, W, C)</span></span>
<span id="cb32-40"><a href="#cb32-40" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb32-41"><a href="#cb32-41" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb32-42"><a href="#cb32-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-43"><a href="#cb32-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the trained generator model</span></span>
<span id="cb32-44"><a href="#cb32-44" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> <span class="st">'/Users/johnaziz/Downloads/generator.pth'</span></span>
<span id="cb32-45"><a href="#cb32-45" aria-hidden="true" tabindex="-1"></a>netG <span class="op">=</span> Generator().to(torch.device(<span class="st">'cpu'</span>))</span>
<span id="cb32-46"><a href="#cb32-46" aria-hidden="true" tabindex="-1"></a>netG.load_state_dict(torch.load(model_path, map_location<span class="op">=</span>torch.device(<span class="st">'cpu'</span>)))</span>
<span id="cb32-47"><a href="#cb32-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Generator model loaded from </span><span class="sc">{</span>model_path<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-48"><a href="#cb32-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-49"><a href="#cb32-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate 16 faces with 4x upscaling (e.g., 32x32 -&gt; 128x128)</span></span>
<span id="cb32-50"><a href="#cb32-50" aria-hidden="true" tabindex="-1"></a>generate_faces(netG, num_images<span class="op">=</span><span class="dv">16</span>, noise_dim<span class="op">=</span><span class="dv">100</span>, device<span class="op">=</span>torch.device(<span class="st">'cpu'</span>), upscale_factor<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb32-51"><a href="#cb32-51" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Generator model loaded from /Users/johnaziz/Downloads/generator.pth</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Computer%20Vision%20Faces_files/figure-html/cell-9-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The model has converged, and it is producing faces, although the quality is more like a horror movie. However as a proof of concept and a basis to iterate upon, the model is working.</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>